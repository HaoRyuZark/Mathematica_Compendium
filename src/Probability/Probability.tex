\newpage
\section{Probability}

\subsection{Basics}

A probability experiment is considered ideal if and only the following properties: 

\begin{itemize}
   
    \item We know the possible outcomes. 

    \item It can be repeated multiple times under the same conditions.

    \item It will be executed under strict conditions.

\end{itemize}

\subsubsection{The Result Set}

The set of all possible results of an experiment is denoted as \(\Omega\) and its cardinality 
is the number of possible outcomes.

For experiments with \(\Omega\) as the result set of one repetition then we define 

\[
    \Omega = \Omega \times \cdots \times \Omega  
\]

As the \emph{result set} of the experiment.

Finally, each subset of \(\Omega\) is called an \emph{outcome}, each subset with 
only one element in it is called an \emph{elemental outcome} The set of all possible outcomes is 
\(\mathcal{P}(\Omega)\)

\subsubsection{Laplace Distribution and Experiment}

A \emph{Laplace distribution} is a probability experiment with finite outcomes where 
each elemental outcome has the same probability of occurring.

\[
    P(\omega) = \frac{\omega}{|\Omega|}
\]

Then an experiment with this property is considered a \emph{Laplace experiment}.

\[
    P(E) = \frac{E}{|\Omega|}
\]


\subsubsection{Expected Result}

Here \(p\) represents the probability of something and \(x\) the expected reward

\[
    E(x) = p_1 x_1 + \cdots + p_n x_n
\]

\subsection{Sigma Algebra}

Given a non-empty set \(\Omega\) and \(\mathcal{A} \subseteq \mathcal{P}(\Omega)\) a set 
of subsets of \(Omega\). \(\mathcal{A}\) is a \(\sigma\)-Algebra if and only if 

\begin{enumerate}[label=\Roman*.]
    
    \item \(\Omega \in \mathcal{A} \) 

    \item \(A \in \mathcal{A} \implies A^c \in \mathcal{A} \)

    \item \(A_1, A_2, A_3, \dots \in \mathcal{A} \implies \bigcup_{n \in \Naturals} A_n \in \mathcal{A} \)

\end{enumerate}

\subsubsection{The Power-set is a Sigma Algebra}

This proof is pretty straight forward. 

For the first axiom, we know that \(\Omega \in \mathcal{P}\). 

For the second axiom, if \(A\) is a subset of the power-set then \(\Omega \setminus A\) is also 
int the power-set.

And lastly, for a sequence of sets in the power-set their, union is also in the power-set, because the power-set 
contains all subsets of \(\Omega\).

\QED

\subsubsection{Properties of Sigma Algebras}

If \(\mathcal{A}\) is a sigma algebra then 

\begin{itemize}
    
    \item \(\emptyset \in \mathcal{A}\) 

    \item \(A_1, A_2, \dots \in \mathcal{A} \implies \bigcap_{n \in \Naturals} A_n \in \mathcal{A}\)

    \item \(A, B \in \mathcal{A} \implies A \setminus B \in \mathcal{A}\)

\end{itemize}

\textbf{Proof:}

For the first, because \(\mathcal{A}\) is a subset of the power-set of \(\Omega\) and the empty set is 
always a subset of a collection, this is trivial. 

Given a number of sets in \(\mathcal{A}\), using the laws of De Morgan we get 

\[
    \bigcap_{n \in \Naturals} A_n = \bigcup A_{n}^{c}
\]

and we know that the complements are part of \(mathcal{A}\) by definition. 

For the last part, \(A \setminus B = A \cap B^c\), because \(B^c \in \mathcal{A}\) we can apply De Morgan 
\((A^c \cup (B^c)^c)^c = (A^c \cup B)^c\) Now because this is the union of two sets in the sigma algebra, and we know by definition 
that their union if also in the set.

\QED

\subsection{Probability Measurement}

Given a non-empty set \(\Omega\) and \(\mathcal{A} \subseteq \mathcal{P}\) a sigma algebra. We define 
the function \(\mathbb{P}:\mathcal{A} \to \Reals\) as a \emph{probability measurement} if 

\begin{enumerate}[label=\Roman*.]
    
    \item \(\forall A \in \mathcal{A}: \mathbb{P}(A) \ge 0\)

    \item \(\mathbb{P}(\Omega) = 1\) 

    \item \(A_n \in \mathcal{A}, n \in \Naturals \text{ disjunctive in pairs } \implies \mathbb{P}\left(\bigcup_{n \in \Naturals} A_n\right) = \sum_{n \in \Naturals} \mathbb{P}(A_n)\) 

\end{enumerate}

Also, the triplet \(\Omega, \mathcal{A}, \mathbb{P}\) is called a \emph{probability space}.

\subsubsection{Probability space of the Power-set}

We want to prove that the probability space with the power-set of omega and \(\mathbb{P}(A) = \frac{|A|}{|\Omega|}\) is in fact a probability space 

\begin{itemize}
    
    \item \(\mathbb{P}(A) = \frac{|A|}{|\Omega|}\) is always greater or equal to zero.
    
    \item \(\mathbb{P}(\Omega) = \frac{|\Omega|}{|\Omega|} = 1\) 

    \item If all of our subsets in the power-set are pairwise disjunctive this mean that their intersection is the empty-set. In other wise there 
          therefore, their independent of each other which means that their probabilities are also independent.

\end{itemize}

\subsubsection{Discrete Probability Spaces}

A probability space \(\Omega, \mathcal{P}(\Omega), \mathbb{P}\) is considered \emph{discrete} if 
\(\Omega\) is finite or infinite countable. In that case \(\Omega = \{\omega_i : i \in I\}\) for \(I = \{1, \dots, n\}\) 

For \(A \in \mathcal{A}\)

\[
    \mathbb{P}(A) = \mathbb{P}\left( \bigcup_{i \in I: \omega_i \in A} \{\omega_i\}\right) = 
    \sum_{i \in I: \omega_i \in A} \mathbb{P}(\{\omega_i\}) = \sum_{i \in I: \omega_i \in A} p(\omega_i)
\]

The function \(\mathbb{P}(\{\omega_i\}) = p(\omega_i)\) is called a \emph{probability function}

To test if specific function or distribution is actually a probability one we have to check.

\begin{itemize}
    
    \item \(\forall p(\omega) \ge 0 \mid \omega \in \Omega\) 

    \item \(\sum_{\omega \in \Omega} p(\omega) = 1\)

\end{itemize}

\subsubsection{Properties of Probability-Measurements}

Given \((\Omega, \mathcal{A}, \mathbb{P})\) and \(A,B \in \mathcal{A}\)


\begin{itemize}

    \item \( \mathbb{P}(\emptyset) = 0 \)
    
    \item \( A_1, \ldots, A_n \in \mathcal{A} \) pairwise disjoint: \( \mathbb{P}\left(\bigcup_{i=1}^n A_i\right) = \sum_{i=1}^n \mathbb{P}(A_i) \)
    
    \item \( \mathbb{P}(B \setminus A) = \mathbb{P}(B) - \mathbb{P}(A \cap B) \)
    
    \item \( \mathbb{P}(A \cup B) = \mathbb{P}(A) + \mathbb{P}(B) - \mathbb{P}(A \cap B) \)
    
    \item \( \mathbb{P}(A^c) = 1 - \mathbb{P}(A) \)
    
    \item If \( A \subset B \), then \( \mathbb{P}(A) \le \mathbb{P}(B) \)
    
    \item \( 0 \le \mathbb{P}(A) \le 1 \)

    \item \( A_1, \ldots, A_n \in \mathcal{A} \): \( \mathbb{P}\left(\bigcup_{i=1}^n A_i\right) \le \sum_{i=1}^n \mathbb{P}(A_i) \)
    
    \item \( A_1, A_2, \ldots \in \mathcal{A} \): \( \mathbb{P}\left(\bigcup_{i=1}^{\infty} A_i\right) \le \sum_{i=1}^{\infty} \mathbb{P}(A_i) \)

\end{itemize}

\textbf{Proof:}

\begin{itemize}

    \item Define \(A_n = \emptyset\) and by the properties of the probability spaces we know that  
    \(A_n \in \mathcal{A}, n \in \Naturals \text{ disjunctive in pairs } \implies \mathbb{P}\left(\bigcup_{n \in \Naturals} A_n\right) = \sum_{n \in \Naturals} \mathbb{P}(A_n)\) 
    
    Therefore, \(\mathbb{P}(\emptyset) = \sum_{i = 1}^{n} \mathbb{P}(A_i)\) and the only real number with the property \(x + x = x\) is 0.
    
    \item By definition  
   
    \item \(B = (A \cap B) \cup B \setminus A\) and by the definition of the addition of probabilities 
    \[
	\mathbb{P}(B) = \mathbb{P}(B \setminus A) - \mathbb{P}(A \cap B)
    \]
    
    \item Using the addition property and by writing the union as \((A \setminus B) \cup (B \setminus A) \cup (A \cap B)\)we get that 

    \[
	\mathbb{P}(A \cup B) = \mathbb{P}(B) - \mathbb{P}(A \cap B) + \mathbb{P}(A) - \mathbb{P}(A \cap B) + \mathbb{P}(A \cup B)	
    \]

    which results in \( \mathbb{P}(A \cup B) = \mathbb{P}(A) + \mathbb{P}(B) - \mathbb{P}(A \cap B) \)
 
    \item \(\Omega = A^c \cup A\) and \(1 = \mathbb{P}(\Omega)\), thus by the addition \(P(A) + P(A^c) = 1\) 
    
    \item Given that if \(A = B\) then the equality holds the case where \(A\) is a subset of \(B\) comes 
    from the fact that it is a strict subset of the other than its cardinality is smaller.
    
    \item By the definition that \(\mathbb{P}(\Omega) = 1\) and the property \(\mathbb{P}(\emptyset) = 0\) 
    we are given an upper and lower bound for our probability distribution. Therefore, every subset of \(\Omega\) will 
    have a probability less or equal to it. For the lower bound the logic is that it has no other subset with fewer elements than it, but 
    it is part of sets with more elements.
    
    \item Let us define \(B_n = A_n \setminus \bigcap{i = 1}_{n - 1} A_i\) which are pairwise disjoint. 
    
    \[
	    \mathbb{P}\left(\bigcup A_n\right) = \mathbb{P}\left(\bigcup B_n\right) = \sum \mathbb{P}\left(B_i\right) \le \sum \mathbb{P}\left(A_i\right)
    \]
    
    Due to the fact that the \(B_i\) are pairwise disjoint.

    \item This is practically the same but for an infinite case.  

\end{itemize}

\QED

\subsection{Inclusion-Exclusion Formula for Two Non-Disjoint Sets}

Let \( |A \cap B| = k \), then:

\[
	|A \cup B| = |A| + |B| - |A \cap B| = n + m - k \quad \text{(since we do not count the intersection 
	twice)}
\]

\subsection{Inclusion-Exclusion Formula for Three Non-Disjoint Sets}

\begin{align*}
	|A \cup B \cup C| &= |(A \cup B) \cup C| \\ 
	&= |A \cup B| + |C| - |(A \cup B) \cap C| \\
 	&= |A| + |B| - |A \cap B| + |C| - |(A \cap C) \cup (B \cap C)|\\
	&= |A| + |B| + |C| - |A \cap B| - |A \cap C| - |B \cap C| + |A \cap B \cap C|
\end{align*}

\subsection{General Formula for the Cardinality of the union of sets}

\[
	\left\vert \bigcup_{i = 1}^n M_i \right\vert  = \sum_{I \subseteq \{1, \dots, n\}, I \neq \emptyset}
	{(-1)}^{|I| - 1} \left\vert \bigcap_{i \in I} M_i \right\vert
\]


\subsection{Stochastic Independence} 

Two results \(A, B \in \mathcal{A}\) are considered \emph{independent} if 

\[
    \mathbb{P}(A \cap B) = \mathbb{P}(A) \cdot \mathbb{P}(B) 
\]

\textbf{Example:}

When throwing two times a dice is the probability of throwing during the first throw an even 
number and when throwing the second dice the sum gives us an even number. 

\[
    \Omega = \{1, \dots, 6\}^2 \quad A = \{2,4,6\} \times \{1, \dots, 6\} \quad B = \{2,4,6\} \cup \{1,3,5\}^2
\]

Therefore, 

\[
    \mathbb{P}(A) = \frac{|A|}{|\Omega|} = \frac{18}{36} = \frac{1}{2}
\]

\[
    \mathbb{P}(B) = \frac{|B|}{|\Omega|} = \frac{18}{36} = \frac{1}{2}
\]

And  

\[
    \mathbb{P}(A \cap B) = \frac{| \{2,4,6\}^2| }{|\Omega|} = \frac{1}{4}
\]

This is just \(\mathbb{P}(A) \cdot \mathbb{P}(B)\). Therefore, they are independent.

\QED 

\subsubsection{Independence for multiple results}

\[
    \mathbb{P}(A_1 \cap A_2 \cap \cdots \cap A_n) = \mathbb{P}(A_1) \cdot \mathbb{P}(A_2) \cdots \mathbb{P}(A_n) 
\]

For all pairs \(A_i, A_j\) where \(i \le j\) 

\subsubsection{Independence in Laplace Distribution}

Give a Laplace distribution \(\mathbb{P} = \frac{|A|}{|\Omega|}\) with two independent results 
\(A\) and \(B\). 

\[
    \frac{|A \cap B|}{|A|} = \frac{B}{|\Omega|}
\]

This means that \(B\) is a probable in \(A\) as in the greater picture.

\subsubsection{Complement Property}

Given the probability space \((\Omega, \mathcal{A}, \mathbb{P})\) and \(A, B \in \mathcal{A}\) as two 
independent results. Then \(A^c\) and \(B\) are also independent.

\textbf{Proof:}

The sets \(A \cap B\) and \(A^c \cap B\) are disjoint and their union is 

\[
    B = (A \cap B) \cup  (A^c \cap B)
\]

If we calculate the probability 

\[
    \mathbb{P}(B) = \mathbb{P}(A \cap B) +  \mathbb{P}(A^c \cap B)
\]

This gives us 

\[
    \mathbb{P}(A^c \cap B) = \mathbb{P}(A) - \mathbb{P}(A \cap B)
\]

Using the Independence of \(A\) and \(B\)

\[
    \mathbb{P}(A^c \cap B) = \mathbb{P}(B) - \mathbb{P}(A)\mathbb{P}(B) = \mathbb{P}(B) (1 - \mathbb{P}(A))
\]

\[
    \mathbb{P}(B) = \mathbb{P}(B) (1 - \mathbb{P}(A)) = \mathbb{P}(B) \cdot \mathbb{P}(A^c)
\]

\QED 

\subsubsection{Independence of Blocks} 

Given \(A_1, \dots, A_n\) independent events, divided into two blocks \(A_1, \dots, A_{k}\) and \(A_{k + 1}, \dots, A_n\). Let 
\(B\) and \(C\) be events resulted of the union, intersection or complement of the first two blocks. Then this results are 
independent

\subsection{The Product Space}

Given two or more probability spaces with their respective tuples of two independent experiments. We define their 
combined probability space as the \emph{product space} where 

\[
    \Omega := \Omega_1 \times \cdots \times \Omega_n
\]

and

\[
    p(\omega_1, \dots, \omega_n) = p(\omega_1) \cdot \dots \cdots p(\omega_n)
\]

An example is throwing two dices at the same time.

\subsection{The Product Space of Discrete Probability Spaces}

Given the product space \((\Omega, \mathcal{P}(\Omega), \mathbb{P})\) of the spaces \((\Omega_i, \mathcal{P}(\Omega_i), \mathbb{P}_i)\) and 
\(A_i \in \mathcal{P}(\Omega_i)\).We define 

\begin{align*}
    A_{i}^{'} &:= \{\omega \in \Omega : \omega_i \in A_i\} \\ 
	      &= \{(w_1, \dots, w_n) : \omega_j \in \Omega_j, \text{ for } j \neq i \text{ and }\omega_i \in A_i\} \\
	      &= \Omega_1 \times \cdots \times \Omega_{i - 1} \times A_i \times \Omega_{i + 1} \times \cdots \times \Omega_n
\end{align*}

For \(i = 1, \dots, n\). Then it holds that \(\mathbb{P}(A_{i}^{'}) = \mathbb{P}_i(A_i)\) and the events \(A_{i}^{'}\) are 
independent.

\subsection{Bernoulli-Experiments}

An experiment where \(\Omega = \{0,1\}, \mathbb{P}(\{1\}) = p\) and \(\mathbb{P}(\{0\}) = 1 - p\) for 
\(p \in (0, 1)\) is called a \emph{Bernoulli-Experiment}.

\subsubsection{Bernoulli Schema}

If we perform a Bernoulli-Experiment \(n\)-times with a success probability of \(p\). Where 
\(\mathbb{P}(A_i) = p\). We define 

\[
    C_k = \bigcup_{I \in \{1, \dots, n\} \land |I| = k} \left(\bigcap_{i \in I} A_i \cap \bigcap_{i \in I} A_{i}^{c}\right)
\]

For the probability we get 

\[
    \mathbb{P}(C_k) = \sum_{I \in \{1, \dots, n\} \land |I| = k} \mathbb{P} \left(\bigcap_{i \in I} A_i \cap \bigcap_{i \notin I} A_{i}^{c}\right) = 
    \sum_{I \in \{1, \dots, n\} \land |I| = k} \prod_{i \in I} \mathbb{P}(A_i) = \prod_{i \notin I} = \binom{n}{k} p^k (1 - p)^{n - k}
\]


\subsubsection{Standard Deviation}

\[
    \sigma = \sqrt{ \frac{ {(\overline{x} - x_1)}^2 + \cdots + {(\overline{x} - x_n)}^2 }{ n } }
\]

\subsection{Binomial Distribution}

\textbf{Formulas:}

\begin{itemize}

    \item \emph{Probability: } \(P(X = k) = \binom{n}{k} p^k {(1 - p)}^{n - k}\)

    \item \emph{Expected Result: } \(E(X) = n * p\)

    \item \emph{Standard Deviation: } \(\sqrt{E(X)(1-p)}\)

    \item  \emph{Variance: } \(E(x)(1-p)\)

\end{itemize}

\subsubsection{Continuous Probability}

\[
    \sum_{i = P(X=k)}^{P(X=n) (P(X=i))}
\]

\textbf{Formulas: }

\begin{itemize}

    \item \(P(X = a) = P(X = a)\)

    \item \(P(X \le a) = P(X \le a)\)

    \item \(P(X < a) = P(X \le a - 1)\)

    \item \(P(X > a) = 1 - P(X \le a)\)

    \item \(P(X \ge a) = 1 - P(X \le - 1)\)

    \item \(P(a \le X \le b) = P(X \le b) - P(X \le a)\)

\end{itemize}

\subsubsection{Sigma Rules}

\begin{itemize}[label = \(-\)]

    \item \(P(\mu - \sigma \le x \mu + \sigma) \approx 68,3\%\)

    \item \(P(\mu - 2\sigma \le x \mu + 2\sigma) \approx 95,4\%\)

    \item \(P(\mu - 3\sigma \le x \mu + 3\sigma) \approx 99,7\%\)

\end{itemize}

\subsection{Conditional Probability}

Given the events \(A\) and \(B\) where \(\mathbb{P}(A) > 0\). The \emph{conditional probability} 
of \(B\) under the condition \(A\) is defined as 

\[
    \mathbb{P}(B \mid A) = \frac{\mathbb{P}(A \cap B)}{\mathbb{P}(A)}
\]

From this definition we can also derive these two statements which are also equivalent. 

\[
    \mathbb{P}(B \mid A) = \mathbb{B} \iff A \text{ and } B \text{ are independent}
\]

\textbf{Proof:}

\begin{align*}	
    \mathbb{P}(B \cap A) = \mathbb{P}(A)\mathbb{P}(B) &= \mathbb{P}(A \cap B){\mathbb{P}(A)}\\ 
    &= \mathbb{P}(B)	
\end{align*}

\QED 

\subsubsection{Conditional Probability of a Subset} 

Given the events \(A\) and \(B\) where \(\mathbb{P}(A) > 0\). If \(A \subset B\) then 
\(\mathbb{P}(B \mid A) = 1\)

\textbf{Proof:}

\(A \subset B \implies A \cap B = A \) therefore 

\[
    \mathbb{P}(B \mid A) = \frac{\mathbb{P}(A)}{\mathbb{P}(A)} = 1
\]

\QED 

\subsubsection{Probability Measurement of Conditional Probability}

Given the probability space \((\Omega, \mathcal{A}, \mathbb{P})\) and \(A \in \mathcal{A}\) where 
\(\mathbb{P}(A) > 0\). The map \(B \to \mathbb{P}(B \mid A)\) is a probability measurement with 
the following properties.

\begin{itemize}
    
    \item \(\forall \in \mathcal{A}: \mathbb{P}(B \mid A) \ge 0\)	
    
    \item \(\mathbb{P}(\Omega \mid A) = 1\)

    \item \(B_n \in A, n \in \Naturals\) pairwise disjoint \(\implies \mathbb{P}\left(\bigcup_{n \in \Naturals} B _n \mid A\right) 
	    = \sum_{n \in \Naturals} \mathbb{P}(B_n \mid A)\)

\end{itemize}

We also get 

\[
    \mathbb{P}(\emptyset \mid A) = 0 
\]

\[
    \mathbb{P}(B^c \mid A) = 1 - \mathbb{P}(B \mid A)
\]

\textbf{Proof:}

\begin{itemize}

    \item Because our sets do not have negative cardinalities and the lowest we can reach is 
	  the probability of the empty-set we can not get below 0.

    \item Similar to another proof we get \(\frac{\mathbb{P}(A)}{\mathbb{P}(A)}\) 

    \item By the definition of probability space.
	
\end{itemize}

\QED

\subsubsection{Multiplication Theorem}

Given \(A_1, dots, A_n\) events with \(\mathbb{P}(A_1 \cap \cdots \cap A_{n - 1}) \ne 0\). Then 

\[
    \mathbb{P}(A_1 \cap \cdots \cap A_n) = \mathbb{P}(A_1)\cdot\mathbb{P}(A_2 \mid A_1) \cdot \dots \cdot \mathbb(A_n \mid A_1 \cap \dots \cap A_{n - 1})
\]

\subsubsection{Theorem of the Total Probability}

Given our standard probability space, the disjoint partitions of \(\Omega\), \(B_1, \dots, B_n\) where 
\(B_1 \cup \dots \cup B_n = \Omega\) and \(\mathbb{P}(B_i) > 0\) then 

\[
    \sum_{i = 1}^{n}\mathbb{P} (A \mid B_i )\mathbb{P}(B_i) \forall A \in \mathcal{A}
\]

\subsection{Multistage Random Experiments}

Let \(\Omega_1, \ldots, \Omega_n\) be discrete sample spaces of \(n\) sub-experiments, and let \(\Omega = \Omega_1 \times \cdots \times \Omega_n\).
Furthermore, let an initial distribution be given by the probability function \(p_1\) with

\[
    \sum_{\omega_1 \in \Omega_1} p_1(\omega_1) = 1
\]

and transition probabilities \(p_j\) for \(j = 2, \ldots, n\) satisfying

\[
    \sum_{\omega_j \in \Omega_j} p_j(\omega_j \mid \omega_1, \ldots, \omega_{j-1}) = 1,
\]

for \((\omega_1, \ldots, \omega_{j-1}) \in \Omega_1 \times \cdots \times \Omega_{j-1}\).

This is used in the case when we model our experiment as a tree of probabilities.

For the multistage random experiment, the following two \emph{path rules} apply:

\begin{enumerate}

    \item Let \((\omega_1, \ldots, \omega_n) \in \Omega\). Then we have:
	\[
	    p(\omega_1, \ldots, \omega_n)
	    = p_1(\omega_1) \cdot p_2(\omega_2 \mid \omega_1) \cdot \ldots \cdot p_n(\omega_n \mid \omega_1, \ldots, \omega_{n-1})
	\]

    \item For \(A \subset \Omega\), it holds that
	\[
	    \mathbb{P}(A) = \sum_{(\omega_1, \ldots, \omega_n) \in A} p(\omega_1, \ldots, \omega_n)
	\]
\end{enumerate}

Note that for independent sub-experiments, we obtain the product space with

\[
    p(\omega_1, \ldots, \omega_n)
    = p_1(\omega_1) \cdot p_2(\omega_2) \cdot \ldots \cdot p_n(\omega_n).
\]

\subsection{Bayes Theorem}

Let \((\Omega, \mathcal{A}, \mathbb{P})\) be a probability space. 
Furthermore, let \(B_1, \ldots, B_n\) be a disjoint partition of the sample space \(\Omega\) with \(\mathbb{P}(B_i) > 0\) for all \(i = 1, \ldots, n\).
Then, for every event \(A \in \mathcal{A}\), the following holds:

\[
    \mathbb{P}(B_i \mid A)
    = \frac{\mathbb{P}(A \mid B_i)\,\mathbb{P}(B_i)}{\mathbb{P}(A)}
    = \frac{\mathbb{P}(A \mid B_i)\,\mathbb{P}(B_i)}{\sum_{k=1}^{n} \mathbb{P}(A \mid B_k)\,\mathbb{P}(B_k)}.
\]


\subsection{Random Variables}

Given a probability space \((\Omega, \mathcal{A}, \mathbb{P})\), we call a function \(f: \Omega \to \Reals\) \emph{measurable} if 
for all \(z \in \Reals\) the following applies 

\[
    f^{-1} ((- \infty, z ]) = \{\omega \in \Omega : f(\omega) \le z \} \in \mathcal{A}
\]

This measurable function \(X: \Omega \to \Reals\) is a \emph{random variable}.

Note that for discrete probability spaces \(\{\omega: X(\omega) \le z\} \subseteq \Omega \in \mathcal{P}\)

Also, it considered discrete if it has only a finite or an infinite countable number of values.

\subsubsection{Operations with Random Variables}

Given two R.V. \(X\),\(Y\) and some number \(\alpha \in \Reals\), then

\begin{itemize}

    \item \(X + Y\) 

    \item \(X - Y\) 

    \item \(X \cdot Y\) 

    \item \(\text{max}(X,Y)\)

    \item \(\text{min}(X,Y)\)

    \item \(\alpha X\) or \(\alpha Y\)

\end{itemize}

are also R.V.

\subsection{Distributions}

Given our probability space and a R.V. \(X\). The probability measurement \(\mathbb{P}_X\) to 
\(\Reals\) defined by 

\[
    \mathbb{P}_X (A) := \mathbb{P}(\{\omega: X(w) \in A\}), \text{ for } A \subseteq \Reals
\]

is called as the \emph{distribution} of \(X\). Also we write \(\mathbb{P}(X \in A)\) as a abbreviation for
\(\mathbb{P}(\{\omega: X(w) \in A\}) \).

\subsection{Distribution Function}

Given a probability space, a probability variable and a distribution \(\mathbb{P}_X\) we define 
the function 

\[
    F_X (x) = \mathbb{P}(X \le x) = \mathbb{P}(\{\omega: X(w) \le x\})
\]

as the \emph{distribution function}.

\subsubsection{Properties of Distribution Functions}

\begin{itemize}
    
    \item \(F_X\) is monotonic growing 

    \item It is right-sided continuous 

    \item \(\lim_{x \to - \infty} F_X (x) = 0\) and \(\lim_{x \to \infty} F_X = 1 \)

\end{itemize}

\subsection{Uniform Distribution}

A random variable \(X \in \{1, 2, \dots, n\}\) with 
\(\mathbb{P}(X = i) = \frac{1}{n}\) for \(i = 1, \dots, n\) is uniformly distributed. 
The distribution \(\mathbb{P}_X\) is called the (discrete) \emph{uniform distribution}.

Notation: \(X \sim U\{1,2,\dots,n\}\).

\subsection{Normal Distribution}

A random variable \(X \in \mathbb{N}_0\) with 
\(\mathbb{P}(X = n) = p(1 - p)^n\) for some \(p \in (0, 1)\) and \(n \in \mathbb{N}_0\) 
is geometrically distributed. The distribution \(\mathbb{P}_X\) is called the \emph{geometric distribution}.

\subsection{Poisson Distribution}

A random variable \(X \in \mathbb{N}_0\) with 

\[
    \mathbb{P}(X = n) = \frac{\lambda^n}{n!} e^{-\lambda}, \quad \text{for some } \lambda > 0,
\] 

is Poisson distributed. The distribution \(\mathbb{P}_X\) is called the \emph{Poisson distribution}.

\subsection{Normal Distribution}

\begin{itemize}
    
    \item \emph{Probability: } \(\frac{1}{\sqrt{2\pi \sigma^2} e^{\frac{1}{2} 
          {\left(\frac{x - u}{\sigma}\right)}^2}}\)
    
    \item \emph{Expected Result: } \(E(x) = np = \mu\)
    
    \item \emph{Variance: } \(Var(x) = E(X) (1-p)\)
    
    \item \emph{Standard Deviation: } \(\sqrt{Var(x)}\)

\end{itemize}

\subsubsection{Formula for the Total Probability}

\[
    P(a) = P_b (a) P(b) + P_{\neg b}(a) P(\neg b)
\]

\subsection{Hyper-geometric Distribution}

\[
    P(X = k) = \frac{\binom{M}{K} \binom{N - M}{n - K}}{\binom{N}{n}}
\]

\textbf{Nomenclature:}

\begin{itemize}

    \item \(N\): Total number of elements

    \item \(M\): Elements with the trait \(A\)

    \item \(N - M\): Elements without the trait \(A\)

    \item \(n = k\): Number to elements to take

\end{itemize}

\textbf{Formulas:}

\begin{itemize}

    \item \emph{Expected Results: } \(E(x) = n \frac{M}{N}\)

    \item \emph{Variance: } \(Var(X) = E(x)\left(1 - \frac{M}{N}\right) \left(\frac{N - n}{N - 1}\right)\)

\end{itemize}

\subsection{The Birthday Paradox}

This is a small question that says: What is the probability of at least two people having 
their birthday on the same day in a group of \(x\) persons.

\[
    P(x) = 1 - \prod_{n = 0}^{x} \frac{365 - n}{365}
\]

This formula gives us the probability of total minus all persons having it on different days. Which
what we are looking for.
