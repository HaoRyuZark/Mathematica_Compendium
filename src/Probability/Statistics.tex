\newpage
\section{Statistics}

\emph{Statistics} focuses on getting information not from probabilities, random variables and 
distributions, but from samples of data collected independently.

\subsection{Basic Nomenclature}

\begin{itemize}

    \item \emph{Population}: Set of all objects about which we make a statement.

    \item \emph{Sample}: Finite set of the Population. 

    \item \emph{Sample Perimeter}: Number of the samples. Like the number of participants. 

    \item \emph{Trait}: Special characteristic we are looking for. They can be discrete, continuous, non-numerical (can be converted to numerical).

    \item \emph{Trait-quantifier}: Amount of correspondence to a trait.
     
\end{itemize}

\subsection{Frequency}

Given a trait \(X\) with some quantifiers \((a_i)_{i \in I}\) for a discrete set \(I\). Given a 
sample of observations \(x_1, x_2, \dots, x_n\) of \(X\). The \emph{absolute frequency} \(h_i\) of \(a_i\) is 
defined as 

\[
    h_i = \sum_{j = 1}^{n} \text{ind}_{x_j = a_i} i \in I
\]

The \emph{relative frequency} is defined as  

\[
    r_i = \frac{h_i}{n}, i \in I
\]

\subsection{Mean}

Given samples \(x_1, \dots, x_n\) the \emph{mean} is given by: 

\[
    \overline{x_n}\frac{1}{n}\sum_{i = 1}^{n} x_i
\]

The \emph{mean} minimizes the quadratic error \(\sum_{i = 1}^{n}(x_i - c)^2\)

\subsection{Order Statistic}

Given \(X_1, \dots, X_n\) random variables. The random variables \(X_{(1)}, \dots, X_{(n)}\) are 
called \emph{order Statistic} of the variables \(X_1, \dots, X_n\) if for each \(\omega \in \Omega\) the following holds: 

\[
    X_{(1)} (\omega) \le X_{(2)} (\omega) \le \dots \le X_{(n)} (\omega)
\]

and for each \(i\) there exist \(k_i\) such that \(X_(i)(\omega) = X_{k_i}(\omega)\).

Note

\[
    X_{(1)} = \min \{X_1, \dots, X_n\}
\]
\[
    X_{(n)} = \max \{X_1, \dots, X_n\}
\]

\textbf{Example:}

Given \(5,1,3,4,2\) we get 

\[
    x_{(1)} = 1, x_{(2)} = 2, \dots, x_{(5)} = 5
\]

\subsection{Median}

Given \(X_1, \dots, X_n\) random variables with their correspondent order statistic. 
Their \emph{empiric median} is defined as 

\[
    \med (X_1, \dots, X_n) = 
    \begin{cases}
        X_{\left(\frac{n + 1}{2}\right)} & n \text{ n is odd} \\ 
        \frac{1}{2} \left(X_{X_{n / 2}} + X_{n /2 + 1}\right) & n \text{ n is odd} \\ 
    \end{cases}
\]

The median is equivariant for addition

\[
    \med(X_1 + a, \dots, X_n + a) = \med(X_1, X_2, \dots, X_n) + a
\]

\textbf{Example:}

Given \(3.80, 4.33, 4.96, 3.71, 3.40\). What is the median?

\emph{Order statistic}: \(3.40, 3.71, 3.80, 4.33, 4.96\)

\emph{Median}: \(n = 5\) thus, \(\frac{n + 1}{2} = 3\) our median is \(\med(X) = X_{(3)} = 3.80\)

\subsection{Empirical Quantile}

Given \(X_1, \dots, X_n\) random variables with their correspondent order statistic. 
The \emph{empirical p-quantile} is defined as 

\[
    q_p = 
    \begin{cases}
    X_{ \left\lfloor  n\cdot p + 1 \right\rfloor}  & \text{ if } np \notin \Naturals \\
    \frac{1}{2} (X_{np} + X_{np + 1})  & \text{ if } np \notin \Naturals 
    \end{cases}
\]

Note 

\[
    \med(X_1, \dots, X_n) = q_{0.5}
\]

\subsection{Modus}

The most recurrent value in a set of samples is called the \emph{modus}. 

Given \(x_1, \dots, x_n\) samples of discrete classes \(c_1, \dots, c_n\). Define \(h_i\) as the 
recurrence of the class \(c_i\) in the sample set. 

\[
    h_i = |\{x_j \in c_i : j = 1, \dots, n\}|
\]

for \(i = 1, \dots, d\). Then is the class \(c_k\) a modus if \(h_k = \max_{i = 1}^{d} h_i\)

\subsection{Empirical Variance}

Let \(X_1, \ldots, X_n\) be random variables with mean \(\overline{X}_n\) 
The \emph{empirical variance} is defined as

\[
    s^2 = \frac{1}{n - 1} \sum_{i = 1}^{n}(X_i - \overline{X_n})^2
\]

\(s\) is called the \emph{empirical standard deviation}.

As a side note, if our samples are distributed uniformly then 

\[
    \Var(X) = \frac{1}{n}\sum_{i = 1}^{n} (x_i - \overline{x_n})^2
\]

\subsection{Sigma Rules}

Given \(X \sim \mathcal{N}(\mu, \sigma^2)\)

\begin{itemize}

    \item \(\Prob(\mu - \sigma \le x \mu + \sigma) \approx 68,3\%\)

    \item \(\Prob(\mu - 2\sigma \le x \mu + 2\sigma) \approx 95,4\%\)

    \item \(\Prob(\mu - 3\sigma \le x \mu + 3\sigma) \approx 99,7\%\)

\end{itemize}

\subsection{Absolute Deviation and Range}

Let\(X_1, \ldots, X_n\) be random variables with mean \(\overline{X}_n\), 
minimum \(X_{(1)}\) and maximum \(X_{(n)}\).

The range \emph{absolute deviation} is defined as

\[
    \mathrm{MAD} = \frac{1}{n} \sum_{i = 1}^{n} |X_i - \overline{X_n}|
\]

The sample \emph{range} is defined as

\[
    r = X_{(n)} - X_{(1)} = \max\{X_1, \ldots, X_n\} - \min\{X_1, \ldots, X_n\}.
\]

\subsection{Interquartile Range}

Let \(X_1, \ldots, X_n\) be random variables with 25\% and 75\% quantiles
\(q_{0.25}\) and \(q_{0.75}\). The \emph{interquartile range} is defined as

\[
    \mathrm{IQR} = q_{0.75} - q_{0.25}.
\]

\subsection{Coefficient of Variation}

Let \(X_1, \ldots, X_n\) be random variables with mean \(\overline{X}_n\) 
and empirical variance \(s^2\). The \emph{empirical coefficient of variation}
is defined as

\[
    V = \frac{s}{\overline{X_n}}
\]

\subsection{Empirical Distribution Function}

Let \(X_1, \ldots, X_n\) be random variables. The function 

\[
    F_n (x) = \frac{1}{n} \sum_{i = 1}^{n} \text{ind}_{X_i \le x}
\]

is the \emph{empirical distribution} function 

\[
    F_n (a_i) = \frac{1}{n} \sum_{j = 1}^{n} \text{ind}_{X_j \le a_i}
\]

If the trait are sorted \(a_1 < a_2 < \dots\) then 

\[
    F_n (a_i) = \frac{1}{n} \sum_{j = 1}^{n} \text{ind}_{X_j \le a_i} = \sum_{k = 1}^{i} \frac{1}{n} \sum_{j = 1}^{n} \text{ind}_{x_j = a_k} = \sum_{k = 1}^{i} r_k
\]

where \(r_k\) is the relative frequency of \(k\)

\textbf{Example:}

Given the samples \(3, 2, 4, 3, 1, 4, 3\) we have the traits \(1, 2, 3, 4\) with relative frequencies 

\begin{itemize}

    \item \(r_1 = \frac{1}{7}\)

    \item \(r_2 = \frac{1}{7}\)

    \item \(r_3 = \frac{3}{7}\)

    \item \(r_4 = \frac{2}{7}\)

\end{itemize}

Then the empirical distribution function is given by

\begin{itemize}

    \item \(F_n(1) = r_1 = \frac{1}{7}\)

    \item \(F_n(2) = r_1 + r_2 = \frac{2}{7}\)

    \item \(F_n(3) = r_1 + r_2 + r_3 = \frac{5}{7}\)

    \item \(F_n(4) = r_1 + r_2 + r_3 + r_4 = 1\)

\end{itemize}

\subsection{Absolute Frequency sum}

\[
    n F_n (a_i) = n \sum_{k = 1}^{i} r_k = \sum_{k = 1}^{i} h_k
\]


where \(h_k\) is the absolute frequency of \(k\)

\subsection{Classified Data}

When dealing with a lot of data sometimes it is easier to put it into
\emph{classes} \(A_1, \dots, A_k\) which represent a certain range \([a,b], [a,b),\) etc. 
with a corresponding \emph{absolute frequency} \(h_i\), \emph{relative frequency} \(r_i\), \emph{class middle} \( \frac{a_{i + 1} - a_{i}}{2} = \alpha_i\).

For the next definitions, the following example table will be used.

\begin{table}[h!]
\centering
\begin{tabular}{c|c|c|c|c}
\textbf{i} & \textbf{Duration} & \(\mathbf{h_i}\) & \(\mathbf{r_i}\) & \(\mathbf{F_i}\) \\ \hline
1 & \([0, 2)\)   & 1650 & 0.3300 & 0.3300 \\
2 & \([2, 4)\)   & 1111 & 0.2222 & 0.5522 \\
3 & \([4, 6)\)   & 720  & 0.1440 & 0.6962 \\
4 & \([6, 8)\)   & 508  & 0.1016 & 0.7978 \\
5 & \([8, 10)\)  & 332  & 0.0664 & 0.8642 \\
6 & \([10, 15)\) & 427  & 0.0854 & 0.9496 \\
7 & \([15, 30]\) & 252  & 0.0504 & 1.0000 \\
\end{tabular}
\end{table}

\(F_i = \sum_{k = 1}^{i} r_k\) 

\emph{Empirical Distribution Function}

\[
    F_n(x) \approx \Prob(X \le x)
\]

\emph{Mean}

\[
    \overline{x} \approx \frac{1}{n} \sum_{i = i}^{k} h_i \alpha_i = \sum_{i = 1}^{k}r_i \alpha_i
\]

\emph{Empirical Variance}

\[
    s^2 \approx \frac{1}{n - 1}\sum_{i = 1}^{k} h_i (\alpha_i - \overline{x_n})^2
\]

\emph{Empirical Quantile}

\[
    q_p = a_i + \frac{p - F_{i - 1}}{F_i - F_{i - 1}}(a_{i + 1} - a_i)
\]

\textbf{Example:}

\(i = 2\), \(A_{2} = [2,4)\) with \(q = 0.5\)

\[
    2 + \frac{0.5 - 0.33}{0.5522 - 0.33}(4 - 2) = 3.53
\]


\emph{Mode Class}

With \(A_i = \text{interval}(a_i, a_{i + 1})\) 

\[
    A_i \text{ with } h_i = \max_{j = 1}^{k} h_j
\]

\subsection{Samples via Random Variables}

Let \(\mathcal{X} \subset \Reals^{n}\) and \(X : \Omega \to \mathcal{X}\) be a 
random variable.  A \emph{sample is a realization} of \(X = (X_{1}, \ldots, X_{n})^{T}\), i.e., \(X(\omega)\) for some \(\omega \in \Omega\).

\begin{itemize}

    \item The set \(\mathcal{X}\) is called the \emph{sample set}.

    \item The samples \(x_1, \dots,,x_n\) correspond to the coordinates of the vector
    \[
        \begin{bmatrix}
            X_1(\omega) \\ 
            \vdots \\ 
            X_n{\omega}
        \end{bmatrix} 
        = 
        X(\omega)
    \]
\end{itemize}

\subsection{Statistical Model}

Let \(\mathcal{X} \subset \Reals^{n}\) and \(X : \Omega \to \mathcal{X}\) be a random variable.

\begin{itemize}

    \item A \emph{statistical model} is a family of probability measures \(P\) on \(\mathcal{X}\).

    \item If \(P = \{ P_{\theta} : \theta \in \Theta \}\), then \(\Theta\) is called the parameter space of \(P\), and \(\theta \in \Theta\) is referred to as a parameter.

\end{itemize}

\textbf{Example:}

\(n\)-fold coin toss  

\begin{itemize}

    \item Sample space: \(\mathcal{X} = \{0,1\}^{n}\)

    \item \(X_{i} \sim \mathrm{Ber}(p)\) (with values in \(\{0,1\}\))

    \item \(X = (X_{1}, \ldots, X_{n})^{T}\) is a random variable in \(\{0,1\}^{n}\)

    \item Sample: \(X(\omega) = (x_{1}, \ldots, x_{n}) \in \{0,1\}^{n}\)

    \item Informally: “Let \(x_{1}, \ldots, x_{n}\) be a sample...”

    \item Marginal distribution of the independent tosses: \(\Prob_{p}(X_{1} = 1) = p\)

    \item Joint distribution: \(\Prob_{p}^{n}\)

    \item Statistical model for the coin toss:

    \[
        P = \{ \Prob_{p}^{n} : p \in (0,1) \}
    \]

\end{itemize}

\subsection{Estimator}

Let \(X : \Omega \to \mathcal{X}\) be a random variable with statistical model
\(P = \{ P_{\theta} : \theta \in \Theta \}\).

\begin{enumerate}

    \item A mapping \(T_{n} : \mathcal{X} \to \Theta\), or the random variable \(T_{n}(X)\),
    is called an \emph{estimator} for \(\theta\).

    \item An estimate is a realization of the estimator, i.e.,
    \(T_{n}(X)(\omega)\) for some \(\omega \in \Omega\).

    \item If for the true distribution of \(X\) we have \(\Prob_{X} = \Prob_{\theta_{0}}\),
    then \(\theta_{0}\) is called the true parameter.

\end{enumerate}

\subsection{Distortion}

Let \(X : \Omega \to \mathcal{X}\) be a random variable with statistical model  
\(P = \{ P_{\theta} : \theta \in \Theta \}\). For \(\theta \in \Theta\), let \( \Expc_{\theta} \) denote the
expectation with respect to \(P_{\theta}\), and let \(\Var_{\theta}\) denote the corresponding variance.

\begin{itemize}

    \item The \emph{bias} of an estimator \(T_{n}\) is
    \[
        \mathrm{Bias}_{T_{n}}(\theta)
        = \Expc_{\theta}[T_{n}(X)] - \theta 
    \]

    \item An estimator is called \emph{unbiased} if
    \(\mathrm{Bias}_{T_{n}}(\theta) = 0\) for all \(\theta \in \Theta\), i.e.,
    \[
        \Expc_{\theta}[T_{n}(X)] = \theta
    \]

    \item A sequence of estimators \((T_{n})_{n \in \mathbb{Z}}\) is called consistent if
    for all \(\varepsilon > 0\) and all \(\theta \in \Theta\),
    \[
        \lim_{n \to \infty} 
        \Prob_{\theta}\!\left( |T_{n}(X) - \theta| > \varepsilon \right)
        = 0 
    \]

\end{itemize}

\subsection{Estimators for Identical, Independent Random Variables}

Let \(X : \Omega \to \mathcal{X}\) be a random variable with  
\(X = (X_{1}, \ldots, X_{n})^{T}\) for independent, identically distributed
random variables \(X_{1}, \ldots, X_{n}\).

\begin{enumerate}

    \item If the expectation of \(X_{1}\) exists, then the sample mean

    \[
        \overline{X}_{n}
    \]

    is an unbiased and consistent estimator for \(\Expc[X_{1}]\).

    \item If the variance of \(X_{1}\) exists and the expectation  
    \(\mu = \Expc[X_{1}]\) is known, then

    \[
        \hat{\sigma}_{n}^{2}
        = \frac{1}{n} \sum_{i=1}^{n} (X_{i} - \mu)^{2}
    \]

    is an unbiased and consistent estimator for \(\Var(X_{1})\).

    \item If the variance of \(X_{1}\) exists and the expectation of \(X_{1}\)
    is unknown, then the sample variance

    \[
        s_{n}^{2}
        = \frac{1}{n-1} \sum_{i=1}^{n} (X_{i} - \overline{X}_{n})^{2}
    \]

    is an unbiased and consistent estimator for \(\Var(X_{1})\).

\end{enumerate}

\subsection{Maximum Likelihood}

Let \(X = (X_{1}, \ldots, X_{n})^{T} : \Omega \to \mathcal{X}\) be a random variable with
statistical model \(P = \{ P^{n}_{\theta} : \theta \in \Theta \}\).
Furthermore, let \(x_{1}, \ldots, x_{n}\) be a sample.
The function \(L : \Theta \to [0,1]\), defined by

\[
    L(\theta) = 
    \begin{cases}
    \Prob_{\theta} (X_1 = x_1) \cdot \dots \cdot \Prob_{\theta} (X_n = x_n) &\text{ for disc. random vars.} \\ 
    f_{\theta} (x_1) \cdot \cdots \cdot f_{\theta} (x_n) &\text{ for cont. random vars.}
    \end{cases}
\]

is called the \emph{likelihood function}.

The parameter \(\hat{\theta}\) that maximizes \(L(\theta)\) is called the
\emph{maximum-likelihood estimate}.

The mapping \(\hat{\theta}(x_{1}, \ldots, x_{n})\) is called the
\emph{maximum-likelihood estimator}.

\subsection{Log-Likelihood-Estimator}

The likelihood function can be pretty challenging to optimize, therefore, 
the less complicated \emph{Log-Likelihood-function} is often used for maximation
problem. 

\[
    l(\theta) = 
    \begin{cases}
        \sum_{i = 1}^{n} \log(\Prob_{\theta} (X_i = x_i)) &\text{ for disc. random vars.} \\
        \sum_{i = 1}^{n} \log( f_\theta(x_i)) &\text{ for cont. random vars.}
    \end{cases}
\]

\subsection{Confidence Interval}

Let \(X = (X_{1}, \ldots, X_{n})^{T} : \Omega \to \mathcal{X}\) be a random variable with
statistical model \(P = \{ \Prob^{n}_{\theta} : \theta \in \Theta \}\).
Furthermore, let \(x_{1}, \ldots, x_{n}\) be a sample and let \(\alpha \in [0,1]\).

A mapping \(I\) that assigns to each sample
\(x = (x_{1}, \ldots, x_{n})^{T} \in X\) a subset \(I(x) \subset \Theta\)
is called a \((1 - \alpha)\)-confidence region for \(\theta\) if

\[
    \Prob_{\theta}\bigl( \theta \in I(X_{1}, \ldots, X_{n}) \bigr)
    \ge 1 - \alpha
\]

for all \(\theta \in \Theta\).

If \(\Theta \subset \Reals\) and \(I(x)\) is an interval for all \(x \in X\),
then \(I\) is called a \((1 - \alpha)\)-\emph{confidence interval}.
