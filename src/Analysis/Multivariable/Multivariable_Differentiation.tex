\newpage
\section{Multi-variable Differentiation}

\subsection{Partial Derivatives}

Let \( f : U \subseteq \Reals^n \to \Reals \) and \( \vec{X}_0 = (x_1^{(0)}, \dots, x_n^{(0)}) 
\in U \). Then \(f\) is \emph{partially differentiable with respect to \( x_i \)} at 
\( \vec{X}_0 \) if the limit:

\[
    \frac{\partial f}{\partial x_i}(\vec{X}_0) := \lim_{h \to 0} \frac{f(x_1^{(0)}, \dots, x_i^{(0)} + h,
     \dots, x_n^{(0)}) - f(\vec{X}_0)}{h}
\]

exists. This derivative measures the rate of change of \(f\) in the direction of the \( x_i \)-axis 
while keeping all other variables fixed.

A function is \emph{(partially) differentiable} at \( \vec{X}_0 \) if all partial derivatives 
\( \frac{\partial f}{\partial x_i}(\vec{X}_0) \) exist.

\subsection{Differentiability}

We call a function \(f\) differentiable at \((x_1, x_2, \cdots, x_n)\) if and only if:

\[
    f(x_1 + \Delta x_1, x_2 + \Delta x_2, \dots, x_n + \Delta x_n) - f(x_1, x_2, \dots, x_n) = 
\]

\[
    \frac{\partial f(x_1, y_2, \dots, x_n)}{\partial x_1} \Delta x_1 + \frac{\partial f(x_1, x_2, 
    \dots, x_n)}{\partial x_2} \Delta x_2 + \cdots + E_1(\Delta x_1) + E_2(\Delta x_n) + \cdots + 
    E_n(\Delta x_n)
\]

where \(\lim_{\Delta x_1 \to 0}\frac{E_1(\Delta x_1)}{\Delta x_1} = \lim_{\Delta x_2 \to 0}
\frac{E_2(\Delta x_2)}{\Delta x_2} = \cdots = 0\)

\subsection{The Gradient}

Let \( f : U \subseteq \Reals^n \to \Reals \) be differentiable at \( \vec{X}_0 = (x_1^{(0)}, 
\dots, x_n^{(0)}) \). The \emph{gradient} of \(f\) at \( \vec{X}_0 \) is:

\[
    \nabla f(\vec{X}_0) = \begin{bmatrix}
    \frac{\partial f}{\partial x_1}(\vec{X}_0) \\
    \vdots \\
    \frac{\partial f}{\partial x_n}(\vec{X}_0)
    \end{bmatrix}
\]

The angle of the maximal slope is \(\phi = \frac{\pi}{2}\). This comes from the fact that 
the directional derivative which is defined as \(\langle \nabla f, \vec{v}\rangle = \|\nabla f\| 
\|\vec{v}\| \cos(\phi)\) is orthogonal to the gradient and this happens at that specific angle. 

And for smallest slope \(\phi = \frac{3 \pi}{4}\). Also, the \emph{gradient} forms a vector field. 

\subsubsection{Origin of the formula}

Suppose along the curve \(r(t) = x_1(t)\hat{v_1} + x_2(t)\hat{v_2} + \cdots + x_n(t)\hat{v_n}\) that 
\(f(x_1(t), \dots, x_n(t)) = C\)

\[
    \frac{\partial}{\partial t}f(x_1(t), \dots, x_n(t)) = \frac{\partial}{\partial t}C
\]

\[
    \frac{\partial f}{\partial x_1}\frac{dx_1}{dt} + \cdots + \frac{\partial f}{\partial x_n}
    \frac{dx_n}{dt} = 0
\]

\[
    \left\langle \left(\frac{\partial f}{\partial x_1}\hat{v_1} + \cdots + \frac{\partial f}{\partial x_n}
    \hat{v_n}\right) , \left( \frac{dx_1}{dt}\hat{v_1} + \cdots + \frac{dx_n}{dt}\hat{v_n}\right)\right\rangle = 0
\]

From the formula we see that: \(\nabla f\) is the normal and \(\frac{d\vec{r}}{dt}\) the tangent vector 
of the curve

\QED

\subsubsection{Gradient Operations:}

\begin{itemize}

    \item \( \nabla(f + g) = \nabla f + \nabla g \)

    \item \( \nabla(\lambda f) = \lambda \nabla f \) for \( \lambda \in \Reals \)

    \item \( \nabla(fg) = f \nabla g + g \nabla f \)

\end{itemize}

The \(\frac{\operatorname{grad}f}{\|\operatorname{grad}f\|}\) points in the direction of the 
steepest increase of \(f\) and \(- \frac{\operatorname{grad}f}{\|\operatorname{grad}f\|}\) in 
the lowest increase.

\subsection{The Tangent Plane}

The tangent plane to a differentiable surface \( z = f(x, y) \) at the point \( (x_0, y_0) \) is the 
plane that best approximates the surface near that point. It is given by the linearization of \(f\):

\[
    T(x, y) = f(x_0, y_0) + f_x(x_0, y_0)(x - x_0) + f_y(x_0, y_0)(y - y_0)
\]

This plane touches the graph of the function at a single point\\
and shares its slope in both \(x\)- and \(y\)-directions.

It can also be written in the parameterized form:

\[
    T(x,y) = \begin{bmatrix} x_0\\ y_0\\ z_0\end{bmatrix} + \lambda 
    \begin{bmatrix} 1\\ 0 \\ f_x(x_0,y_0)\end{bmatrix} + \mu \begin{bmatrix}
    0 \\ 1 \\ f_y(x_0, y_0)
    \end{bmatrix}
\]

\subsubsection{Generalization}

\[
    T(\vec{X}) = f(\vec{x_0}) + \sum_{i = 1}^{n} f_{x_i}(\vec{x_0})(x_i - x_{i}^0)
\]
\[
    = f(\vec{x_0}) + \langle \nabla f, (\vec{x} - \vec{x_0})\rangle
\]

\textbf{Example: \( f(x, y) = 4x^2y + xy^2 + 1 \)}

Compute partial derivatives:

\[
    f_x(x, y) = 8xy + y^2, \quad f_y(x, y) = 4x^2 + 2xy
\]

At the point \( (1, 1) \):

\[
    f(1, 1) = 4{(1)}^2(1) + 1{(1)}^2 + 1 = 6 \\
    f_x(1, 1) = 8(1)(1) + 1 = 9, \quad f_y(1, 1) = 4{(1)}^2 + 2(1)(1) = 6
\]

Thus, the tangent plane is:

\[
    T(x, y) = 6 + 9(x - 1) + 6(y - 1)
    \Rightarrow T(x, y) = 9x + 6y - 9
\]

\subsection{The Directional Derivative}

Let \( f : \Reals^n \to \Reals \) be differentiable at \( \vec{x}_0 \), 
and let \( \vec{v} \in \Reals^n \) be a direction vector with \( \|\vec{v}\| = 1 \). The 
\emph{directional derivative} of \(f\) at \( \vec{x}_0 \) in the direction \( \vec{v} \) is defined as:

\[
    D_{\vec{v}}f(\vec{x}_0) := \lim_{h \to 0} \frac{f(\vec{x}_0 + h\vec{v}) - f(\vec{x}_0)}{h}
\]

\textbf{Formula:}

\[
    D_{\vec{v}}f(\vec{x}_0) = \langle \nabla f(\vec{x}_0), \vec{v} \rangle
\]

\emph{Schwarz Inequality (Cauchy–Schwarz):}

\[
    |\langle \vec{a}, \vec{b} \rangle| \le \|\vec{a}\| \cdot \|\vec{b}\|
\]

This implies:

\[
    |D_{\vec{v}}f(\vec{x}_0)| \le \|\nabla f(\vec{x}_0)\|
\]

\subsubsection{Derivation of the Directional Derivative Formula}

Let \( f(x, y) \) be differentiable, and let \( \vec{v} = (e_1, e_2) \) be a direction vector. We aim to 
derive the formula for the directional derivative \( D_{\vec{v}} f(x_0, y_0) \).

We begin by considering the tangent plane to the surface \( z = f(x, y) \) at the point 
\( (x_0, y_0, f(x_0, y_0)) \). This plane is spanned by the vectors:

\[
    \vec{u}_1 = 
    \begin{bmatrix}
    1 \\
    0 \\
    f_x(x_0, y_0)
    \end{bmatrix}, \quad
    \vec{u}_2 = 
    \begin{bmatrix}
    0 \\
    1 \\
    f_y(x_0, y_0)
    \end{bmatrix}
\]

These correspond to directional derivatives along the \(x\)- and \(y\)-axes, respectively.

Now consider the direction vector \( \vec{v} = (e_1, e_2) \). Lift it into 3D space (onto the tangent 
plane) as:

\[
    \vec{u}_3 =
    \begin{bmatrix}
    e_1 \\
    e_2 \\
    D_{\vec{v}}f
    \end{bmatrix}
\]

Since all three vectors lie in the same plane, the determinant of the matrix formed by these vectors as 
columns must vanish:

\[
    \det(\vec{u}_1, \vec{u}_2, \vec{u}_3) = 0
\]

Explicitly:

\[
    \det
    \begin{bmatrix}
    1 & 0 & e_1 \\
    0 & 1 & e_2 \\
    f_x(x_0, y_0) & f_y(x_0, y_0) & D_{\vec{v}}f
    \end{bmatrix} = 0
\]

Expanding the determinant gives:

\[
    f_x(x_0, y_0) \cdot e_2 - f_y(x_0, y_0) \cdot e_1 + D_{\vec{v}}f = 0
\]

Solving for \( D_{\vec{v}}f \), we obtain:

\[
    D_{\vec{v}}f = f_x(x_0, y_0) \cdot e_1 + f_y(x_0, y_0) \cdot e_2
\]

\textbf{Vector notation:}

\[
    D_{\vec{v}}f = \langle \nabla f(x_0, y_0), \vec{v} \rangle
\]

This is the desired result: the directional derivative equals the dot product of the gradient of \(f\) 
and the direction vector \( \vec{v} \).

\subsection{The Total Differential}

If \( f : \Reals^n \to \Reals \) is differentiable at \( \vec{x}_0 \), then the 
\emph{total differential} of \(f\) at \( \vec{x}_0 \) is the linear approximation:

\[
    df = \sum_{i=1}^n \frac{\partial f}{\partial x_i}(\vec{x}_0) \, dx_i = \langle \nabla f(\vec{x}_0), 
    d\vec{x} \rangle
\]

This expresses how small changes in the input variables propagate into changes in the function values.

\subsection{The Chain Rule}

Let \( f : \Reals^n \to \Reals \) be differentiable, and suppose \( \vec{x} = \vec{x}(t) \in \Reals^n \) 
is a differentiable path. Then:

\[
    \frac{d}{dt} f(\vec{x}(t)) = \left\langle \nabla f(\vec{x}(t)), \frac{d\vec{x}(t)}{dt} \right\rangle
\]

This is the chain rule in vector form.

Or to be more explicit, the small change in \(t\) makes a change in \(x_1, x_2, \cdots\) and the addition 
of all this changes sum up to the total change in the original function.

\[
    \frac{\partial z}{\partial t} = \frac{\partial z}{\partial x_1}\frac{\partial x_1}{ \partial t} + 
    \cdots + \frac{\partial z}{\partial x_n}\frac{\partial x_n}{\partial t}
\]

\subsection{Absolute and Relative Error}

Given an approximate value \( \tilde{x} \) for the exact value \(x\):

\begin{itemize}

    \item \emph{Absolute error:} \(\Delta z_{\max} \le |f_{x_1}||\Delta x_1| + \cdots + |f_{x_n}|
          |\Delta x_n| \)

    \item \emph{Relative error:} \( \varepsilon = \frac{\Delta z_{\max}}{z}\) with dependence on the error 
          of the inputs \(\frac{\Delta x}{x}\) etc.

\end{itemize}

In multi-variable contexts, similar formulas apply using vector norms.

\textbf{Absolute Error Example}

We consider the function:

\[
    z = \sqrt{x^2 + y^2}
\]

with the measured values:

\[
    x = 4 \, \text{cm}, \quad y = 3 \, \text{cm}
\]

Both \(x\) and \(y\) are measured with a precision of \( \Delta x = \Delta y = 0{,}1 \, \text{cm} \). 
The side length calculated is \( z = 5 \, \text{cm} \). We now determine the maximum possible absolute 
error in \(z\).

\textbf{Step 1: Partial derivatives of \(z\):}

\[
    \frac{\partial z}{\partial x} = \frac{x}{\sqrt{x^2 + y^2}}, \quad
    \frac{\partial z}{\partial y} = \frac{y}{\sqrt{x^2 + y^2}}
\]

\textbf{Step 2: Evaluate at the given point:}

\[
    \frac{\partial z}{\partial x}(4, 3) = \frac{4}{5}, \quad
    \frac{\partial z}{\partial y}(4, 3) = \frac{3}{5}
\]

\textbf{Step 3: Use the absolute error formula:}

\[
    \Delta z \leq \left| \frac{\partial z}{\partial x} \right| \cdot \Delta x + \left| \frac{\partial z}
    {\partial y} \right| \cdot \Delta y
\]
\[
    \Delta z \leq \frac{4}{5} \cdot 0{,}1 + \frac{3}{5} \cdot 0{,}1 = \frac{7}{5} \cdot 0{,}1 = 0{,}14 \, 
    \text{cm}
\]

\textbf{Conclusion:}  

The result for \(z\) is accurate to within \( 0,14 \text{cm} \).

\textbf{Relative Error Example}

The relative error of a function \( z = f(x_1, \dots, x_n) \) is defined by:

\[
    \varepsilon = \frac{\Delta z_{\max}}{z}
\]

Assuming \(z\) depends on variables with known relative measurement errors, and the function is 
differentiable, we estimate:

\[
    \varepsilon = \frac{\left| \frac{\partial f}{\partial x_1} \right| \cdot 
    \Delta x_1 + \cdots + \left| \frac{\partial f}{ 
    \partial x_n}\right| \cdot \Delta x_n}{f(x_1, \dots, x_n)}
\]

or more directly using relative errors:

\[
    \varepsilon \approx \left| \frac{\Delta x_1}{x_1} \right| + \cdots + \left| \frac{\Delta x_n}{x_n} 
    \right|
\]

\textbf{Example: Volume of a Cuboid}

Let:

\[
    z = f(a, b, c) = a \cdot b \cdot c
\]

Assume the side lengths \( a = b = c \) are measured with a uniform relative error 
\( \varepsilon_{\text{in}} = \frac{\Delta a}{a} = \frac{\Delta b}{b} = \frac{\Delta c}{c} = \varepsilon \)

Then the partial derivatives are:

\[
    \frac{\partial z}{\partial a} = b c, \quad
    \frac{\partial z}{\partial b} = a c, \quad
    \frac{\partial z}{\partial c} = a b
\]

Applying the relative error formula:

\[
    \frac{\Delta z_{\max}}{z} \leq \frac{bc \cdot \Delta a + ac \cdot \Delta b + ab \cdot \Delta c}{abc}
    = \frac{\Delta a}{a} + \frac{\Delta b}{b} + \frac{\Delta c}{c} = 3 \varepsilon
\]

\textbf{Target accuracy:}  

If the final result \(z\) must be accurate within \( \varepsilon_z = 0{,}01 \) (i.e., 1), then we must 
satisfy:

\[
    3 \varepsilon \leq 0{,}01 \Rightarrow \varepsilon \leq \frac{0{,}01}{3} = 0,0033 \quad 
    \text{(or } 0,33\% \text{)}
\]

\textbf{Conclusion:}

Each input measurement must be made with a maximum relative error of \( 0,33\% \) to ensure 
the output \( z = abc \) is accurate to within 1 percent.

\subsection{Implicit Differentiation}

If a function \( F(x, y) = 0 \) defines \(y\) implicitly as a function of \(x\), and \(F\) is 
differentiable, then:

\[
    \frac{dz}{dx} = -\frac{F_x}{F_z}
\]

This generalizes to higher dimensions using the total differential and the implicit function theorem.

\textbf{Example:}

Given is \(x^2 + y^2 + z^2 = 1\) differentiate with respect to \(x\).
See \(z\) or the target function as function in terms of the other variables. 
In this case \(z(x,y)\).

\[
    \frac{d}{dx} x^2 + y^2 + z^2  -1 = 0
\]

\[
    2x + 2z \frac{dz}{dx}  = 0
\]

\[
    \frac{dz}{dx} = - \frac{x}{z}
\]

\textbf{Example:}

Given the implicitly defined function:

\[
    y x^2 + \frac{x}{y} + x + y - 4 = 0
\]

We are asked to compute the derivative \( y'(1) \), that is, the derivative of \(y\) 
with respect to \(y\) at the point \( (1, 1) \).

\textbf{Step 1: Differentiate implicitly}

Differentiate both sides of the equation with respect to \(x\), 
treating \( y = y(x) \) as a function of \(x\):

\begin{align*}
    \frac{d}{dx}(y x^2) + \frac{d}{dx}\left(\frac{x}{y}\right) + \frac{d}{dx}(x) + \frac{d}{dx}(y) - \frac{d}{dx}(4) &= 0 \\
    y' x^2 + y \cdot 2x + \frac{y - x y'}{y^2} + 1 + y' &= 0
\end{align*}

Collect terms involving \( y' \). Group all \( y' \) terms on one side:

\[
    y' x^2 - \frac{x y'}{y^2} + y' + 2xy + \frac{1}{y} + 1 = 0
\]

Factor \( y' \):

\[
    y' \left(x^2 - \frac{x}{y^2} + 1\right) = -\left(2xy + \frac{1}{y} + 1\right)
\]

Evaluate at \( x = 1, y = 1 \)

Substitute \( x = 1 \), \( y = 1 \):

\begin{align*}
    y' (1 - 1 + 1) &= - (2 + 1 + 1) \\
    y' (1) &= -4 \\
    y' &= -4
\end{align*}

Final Answer

\[
    y'(1) = -4
\]

\subsection{Divergence and Curl}

Let \( \vec{F} = (F_1, F_2, F_3) : \Reals^3 \to \Reals^3 \) be a vector field.

\subsubsection{Divergence:}

\[
    \operatorname{div} \vec{F} = \langle\nabla, \vec{F}\rangle = \frac{\partial F_1}{\partial x_1} + 
    \frac{\partial F_2}{\partial x_2} + \frac{\partial F_3}{\partial x_3}
\]

\subsubsection{Curl:}

\[
    \operatorname{rot} \vec{F} = \nabla \times \vec{F} = \begin{bmatrix}
    \frac{\partial F_3}{\partial x_2} - \frac{\partial F_2}{\partial x_3} \\
    \frac{\partial F_1}{\partial x_3} - \frac{\partial F_3}{\partial x_1} \\
    \frac{\partial F_2}{\partial x_1} - \frac{\partial F_1}{\partial x_2}
    \end{bmatrix}
\]

These describe how the field spreads or rotates around a point.

Where vector field \( \operatorname{div}\vec{f} > 0\) then that region is a called a source
and where \(\operatorname{rot}\vec{f} = 0\) is called a whirlpool

\subsection{Schwarz’s Theorem (Clairaut’s Theorem)}

Let \( f : \Reals^n \to \Reals \) be twice continuously differentiable. Then for any \( i \ne j \), 
the mixed partial derivatives satisfy:

\[
    \frac{\partial^2 f}{\partial x_i \partial x_j} = \frac{\partial^2 f}{\partial x_j \partial x_i}
\]

That is, the order of partial differentiation does not matter if all second derivatives are continuous.

\subsection{The Jacobian}

Let \( \vec{f} : \Reals^n \to \Reals^m \), where

\[
    \vec{f}(\vec{x}) = \begin{bmatrix}
    f_1(x_1, \dots, x_n) \\
    \vdots \\
    f_m(x_1, \dots, x_n)
    \end{bmatrix}
\]

The \emph{Jacobian matrix} of \( \vec{f} \) is the \( m \times n \) matrix:

\[
    J_{\vec{f}}(\vec{x}) = \begin{bmatrix}
    \frac{\partial f_1}{\partial x_1} & \cdots & \frac{\partial f_1}{\partial x_n} \\
    \vdots & \ddots & \vdots \\
    \frac{\partial f_m}{\partial x_1} & \cdots & \frac{\partial f_m}{\partial x_n}
    \end{bmatrix}
\]

In the case \( n = m \), the \emph{Jacobian determinant} \( \det J_{\vec{f}}(\vec{x}) \) 
describes local invertibility and orientation.

\subsection{Taylor's Theorem}

Let \( f : \Reals^n \to \Reals \) be \(k\)-times continuously differentiable at \( \vec{x}_0 \). Then 
the Taylor polynomial of degree 2 around \( \vec{x}_0 \) is:

\[
    f(\vec{x}) \approx f(\vec{x}_0) + \langle \nabla f(\vec{x}_0), \vec{x} - \vec{x}_0 \rangle + 
    \frac{1}{2} {(\vec{x} - \vec{x}_0)}^T H_f(\vec{x}_0)(\vec{x} - \vec{x}_0) + \cdots,
\]

where \( H_f(\vec{x}_0) \) is the \emph{Hessian matrix} of second partial derivatives.

\textbf{Example:} 

Let \( f(x, y) = e^y \sin(x + 2y) \). We expand around the point \( (a, b) = (0, 0) \).

First, compute the necessary derivatives at \( (0, 0) \):

\begin{align*}
    f(0, 0) &= \sin(0) = 0 \\
    f_x &= e^y \cos(x + 2y) \quad \Rightarrow \quad f_x(0, 0) = 1 \\
    f_y &= 2y e^y \cos(x + 2y) + e^y \sin(x + 2y) \quad \Rightarrow \quad f_y(0, 0) = 0 + 2 = 2 \\
    f_{xx} &= -e^y \sin(x + 2y) \quad \Rightarrow \quad f_{xx}(0, 0) = 0 \\
    f_{xy} = f_{yx} &= 2e^y \sin(x + 2y) + e^y \cos(x +2y) \quad \Rightarrow \quad f_{xy}(0, 0) = 3 \\
    f_{yy} &= -3 e^y \sin(x + 2y) + 4e^y \cos(x + 2y) \quad \Rightarrow \quad f_{yy}(0, 0) = 0 + 4 = 4
\end{align*}

Now, the second order Taylor polynomial at \( (0, 0) \) is:

\begin{align*}
    f(x, y) \approx\ & 0 + 1 \cdot x + 2 \cdot y \\
    &+ \frac{1}{2} \cdot 0 \cdot x^2 + 3 \cdot x y + \frac{1}{2} \cdot 4 \cdot y^2 \\
    =\, & x + 2y + 3xy + 2y^2
\end{align*}

\subsection{Relative Extrema}

To determine local extrema for \( f : \Reals^n \to \Reals \), follow this procedure:

\textbf{Step 1:} Find critical points by solving:

\[
    \nabla f(\vec{x}) = 0
\]

\textbf{Step 2:} Compute the Hessian matrix:

\[
    H_f(\vec{x}) = \left( \frac{\partial^2 f}{\partial x_i \partial x_j} \right)
\]

\textbf{Step 3:} Analyze the Hessian at critical points:

\begin{itemize}

    \item If \( H \) is positive definite \(f_{xx}(x_0, y_0) > 0, \det(H) > 0 \implies\) local minimum

    \item If \( H \) is negative definite \(f_{xx}(x_0, y_0) < 0, \det(H) > 0 \implies\) local maximum

    \item If \( H \) has mixed signs (indefinite) \(\det(H) < 0 \implies  \) saddle point

    \item If \(\det(H) = 0 \implies \) Next derivative decides

\end{itemize}

The determinant is needed as part of the test because, while the second derive tells us about the concavity, the 
determinant help us determine if all the concavities given by the axis second derivatives with respect to 
\(x_1, x_2, \dots, x_n\) and the mix partials are up, down or neither. If the signs are mixed then it is a saddle.  

\textbf{Example: \( f(x, y) = x^2 + y^2 \)}

\[
    \nabla f = \begin{bmatrix} 2x \\ 2y \end{bmatrix} \Rightarrow \text{Critical point at } (0, 0)
\]

\[
    H_f = \begin{bmatrix} 2 & 0 \\ 0 & 2 \end{bmatrix} \Rightarrow \text{positive definite}
    \Rightarrow (0, 0) \text{ is a local minimum}
\]

\subsection{Finding Candidate Points for Zeros While Seeking Relative Extrema}

To find local minima or maxima of a function \( f : \Reals^n \to \Reals \), we first identify 
\emph{critical points}, which are the candidates for relative extrema. 
These are the points where the gradient of \(f\) vanishes or does not exist.

\textbf{Step 1: Compute the Gradient}

The gradient of \(f\), denoted \( \nabla f \), collects all the first partial derivatives:

\[
    \nabla f(x_1, \dots, x_n) =
    \begin{bmatrix}
    \frac{\partial f}{\partial x_1} \\
    \vdots \\
    \frac{\partial f}{\partial x_n}
    \end{bmatrix}
\]

\textbf{Step 2: Solve the System \( \nabla f = 0 \)}

Find all points \( \vec{x}_0 \in \Reals^n \) such that:

\[
    \nabla f(\vec{x}_0) = \vec{0}
\]

This is a nonlinear system of \(n\) equations in \(n\) variables. The solutions \( \vec{x}_0 \) 
are the \emph{candidate points for extrema}, and also referred to as the 
\textbf{stationary points} or \emph{critical points} of the function.

You use the following techniques to find the points: 

\begin{itemize}

    \item Factorization

    \item Addition of rows

    \item Substitution

\end{itemize}

There are more but which one to use depends on the kind problem.

\textbf{Step 3: Analyze the Nature of the Critical Points}

Once all critical points \( \vec{x}_0 \) satisfying \( \nabla f(\vec{x}_0) = 0 \) are found, analyze 
each using the \emph{second derivative test} involving the Hessian matrix \( H_f \):

\[
    H_f(\vec{x}) = \left( \frac{\partial^2 f}{\partial x_i \partial x_j} \right)
\]

Evaluate \( H_f \) at each critical point and use the eigenvalues or definiteness to classify:

\begin{itemize}

    \item If \( f_{xx}(x_0, y_0) > 0 \) and \( D > 0 \), then \(f\) has a \emph{local minimum} at 
          \( (x_0, y_0) \).

    \item If \( f_{xx}(x_0, y_0) < 0 \) and \( D > 0 \), then \(f\) has a \emph{local maximum} at 
          \( (x_0, y_0) \).

    \item If \( D < 0 \), then \( (x_0, y_0) \) is a \emph{saddle point}.

    \item If \( D = 0 \), the \emph{second derivative test is inconclusive}; higher-order derivatives 
          must be considered.

\end{itemize}

\subsubsection{Note: Non-differentiable Points}

If the function \(f\) is not differentiable at some point \( \vec{x}_0 \), but defined there, this 
point may also be a candidate for extremum and should be examined separately using limit-based analysis 
or directional behavior.

\subsection{Lagrange Multipliers}

Let \( f : \Reals^n \to \Reals \) be the function to optimize, subject to the constraint 
\( g(\vec{x}) = 0 \), where \( g : \Reals^n \to \Reals \).  

If we look at the constraint intersection with the original function we are going to notice that along 
the intersection there are maxima and minima with their respective tangent lines and gradients that are 
normal to them. We also are going to see that both the gradient of \(f\) and the gradient of \(g\) are just scalar version of each other so: 

This leads to the \emph{Lagrange system}:

\[
    \begin{cases}
        \nabla f(\vec{x}) = \lambda \nabla g(\vec{x}) \\
        g(\vec{x}) = 0
    \end{cases}
\]

For more conditions  we can use \(g_1, \dots, g_n\) and \(\lambda_1, \dots, \lambda_n\)

\[
    \begin{cases}
        \nabla f(\vec{x}) = \lambda_1 \nabla g_1(\vec{x}) + \cdots + \lambda_n \nabla g_n(\vec{c}) \\
        g_1(\vec{x}) = 0 \\
        \vdots \\
        g_n(\vec{x}) = 0
    \end{cases}
\]

We can also compact these equations in the \emph{Lagrangian}

\[
    \mathcal{L}(x,y, \dots, \lambda_1, \dots, \lambda_n) = f(x, y, \dots) + \lambda_1 g_1(x, y, \dots) + 
    \cdots + \lambda_n g_n(x, y, \dots)
\]

\subsubsection{Determinant method}

Sometimes it is a good way to solve a problem of this topic is to:

\begin{enumerate}
    
    \item Build the \emph{Lagrangian}
    
    \item \(\nabla L = 0\) and maybe compute the determinant to generate another equation to solve
        
    \item Get rid of \(\lambda\) via factorization, substitution, addition of rows, etc.
    
    \item Solve the system without \(\lambda\)

\end{enumerate}

\subsubsection{The Bordered Hessian}

The Bordered Hessian matrix is:

\[
    H_B =
    \begin{bmatrix}
    0 & {\left( \nabla g(x_1, \dots, x_n) \right)}^T \\
    \nabla g(x_1, \dots, x_n) & \nabla^2 \mathcal{L}(x, \dots, x_n, \lambda_1, \dots, \lambda_n)
    \end{bmatrix}
\]

Where:

\begin{itemize}

    \item \( \nabla  g(x_1, \dots, x_n) \) is the \( m \times n \) Jacobian matrix of the constraints.

    \item \( \nabla^2 \mathcal{L}(x, \dots, x_n, \lambda_1, \dots, \lambda_n) \) is the \( n \times n \) 
          Hessian of the Lagrangian with respect to \(x\).

\end{itemize}

\textbf{Example I:}

Find the point on the circle \(x^2 + y^2 = 4\) that is the closest to the point \((3,4)\).

The function \(f(x,y)\) is this case the euclidean norm:

\[
    f(x, y) = \sqrt{(x - 3)^2 + (y - 4)^2}
\]

subject to the constraint:

\[
    g(x, y) = x^2 + y^2 - 4 = 0
\]

The gradient of the constraint is:

\[
    \nabla g = \begin{bmatrix} 2x \\ 2y \end{bmatrix}
\]

Since \( \nabla g \ne 0 \), the rank condition is fulfilled, and no further critical points 
need to be analyzed.

\textbf{Step 1: Build the Lagrangian}

\[
    \mathcal{L}(x, y, \lambda) = f(x, y) + \lambda g(x, y)
\]

In this case we can use a property of the root the function to our advantage. Notice that the minimum 
of the root function is the minimum of the term under root therefore, we can simplify some work by writing 
\(f(x,y)\) without the root. 

\[
    \mathcal{L}(x, y, \lambda) = (x - 3)^2 + (y - 4)^2 + \lambda (x^2 + y^2 - 4)
\]

\textbf{Step 2: Compute the necessary conditions}

\begin{align*}
   \textbf{I } &\frac{\partial \mathcal{L}}{\partial x} = 2(x - 3) + 2\lambda x = 0 \\
   \textbf{II } &\frac{\partial \mathcal{L}}{\partial y} = 2(y - 4) + 2\lambda y = 0 \\
   \textbf{III } &\frac{\partial \mathcal{L}}{\partial \lambda} = x^2 + y^2 - 4 = 0
\end{align*}

Now, this is the difficult part. The best approach in this case is to use substitution to 
with a top-bottom approach:

\[
    x = \frac{3}{1 + \lambda}
\]

\[
    y = \frac{4}{1 + \lambda}
\]

Solving for \(\lambda\) is also an option but in these kinds of systems of equations where 
both the partial with respect to \(x\) and \(y\) are very similar it is a good idea to solve for them 
because they tend to have a similar form when solving for them.

\begin{align*}
    \left(\frac{3}{1 + \lambda}\right)^2 + \left(\frac{4}{1 + \lambda}\right)^2 - 4 &= 0\\
    \frac{9}{(1 + \lambda)^2} + \frac{16}{(1 + \lambda)^2} - 4 &= 0\\
    \frac{25}{(1 + \lambda)^2} &= 4 \\
    25 &= 4 (1 + \lambda)^2 \\
    \sqrt{\frac{25}{4}} &= 1 + \lambda \\
    \lambda &= -1 \pm \frac{5}{2}
\end{align*}

\[
    \lambda_1 = \frac{-7}{2} \quad \lambda_2 = \frac{3}{2}
\]

Now, let us substitute backwards for \(\lambda_1\):

\[
    x = \frac{3}{1 - \frac{7}{2}} = \frac{6}{-5}
\]

\[
    y = \frac{4}{1 - \frac{7}{2}} = \frac{8}{-5}
\]

Now,  for \(\lambda_2\):

\[
    x = \frac{3}{1 + \frac{3}{2}} = \frac{6}{5}
\]

\[
    y = \frac{4}{1 + \frac{3}{2}} = \frac{8}{5}
\]

\textbf{Step 3: Hessian Test with Constraint}

We construct the Hessian:

\[
    H = \begin{bmatrix}
    2 + 2\lambda & 0 & 2x \\
    0 & 2 + 2\lambda & 2 \\
    2x & 2 & 0
    \end{bmatrix}
\]

For \(\lambda_1\) we get \( \det(H) =  \frac{244}{5} > 0 \), a minimum 
and for \(\lambda_2\) \(\det(H) = \frac{-244}{5}\) a maximum.

\textbf{Example II:}

Optimize the distance to the origin \(f(x,y,z)\) with respect to the constraints 
\(g_1(x,y,z) = z^2 - x^2 - y^2 = 0\) and \(g_2(x,y,z) = x - 2z - 3 = 0\)

\begin{align*}
    \nabla f &= \lambda \nabla g_1 + \mu \nabla g_2 \\
    g_1 &= 0 \\
    g_2 &= 0 
\end{align*}

We know that \(f(x,y,z) = \sqrt{x^2 + y^2 + z^2}\). Like with other minimization problems we can 
just find the minimum of the part under root thus, \(f(x,y,z) = x^2 + y^2 + z^2\). Now we can put 
the build our system of equations

\begin{align*}
    \textbf{I   } &2x = \lambda(-2x) + \mu \\
    \textbf{II  } &2y = \lambda(-2y) \\
    \textbf{III } &2z = \lambda(2z) + \mu(-2) \\
    \textbf{IV  } &z^2 = x^2 + y^2 \\
    \textbf{V   } &x = 2z + 3   
\end{align*}

For the equation \textbf{II} we can already see that the equation is equal to zero only if 
\(\lambda = -1\) or \(y = 0\). Thus, we have got our first numbers to work with.

\textbf{Case \(\lambda = -1\):}

\begin{align*}
    \textbf{I   } &2x = -1(-2x) + \mu  \implies \mu = 0\\
    \textbf{III } &2z = -1(2z) + 0(-2) \implies z = 0 \\
    \textbf{IV  } &0^2 = x^2 + y^2  \implies x = y = 0\\
    \textbf{V   } &0 = 0 + 3 \implies 0 = 3   
\end{align*}

With the last contradiction we know that the whole case \(\lambda = -1\) is not valid.

\textbf{Case \(y = 0\):}

\begin{align*}
    \textbf{IV  } &z^2 = x^2 + y^2 \implies z^2 = x^2  \implies z = \pm x\\
    \textbf{V   } &x = 2x + 3  \implies x = -3 (z = x)\\ 
    \textbf{V   } &x = -2x + 3  \implies x = 1 (z = -x)\\ 
\end{align*}

Now with this new information we can get two different points \(P_1(1, 0, -1)\) and \(P_2(-3, 0, -3)\).
We do not need to write the Hessian in this case because with geometric intuition the problem can be 
solved.

\[
    f(P_1) = 1^2 + 0^2 + (-1)^2 = 2 
\]

So the distance is \(\sqrt{2}\).

\[
    f(P_2) = -3^2 + 0^2 -3^2 = 18
\]

Thus, the distance is \(3\sqrt{2}\).

Our final Answer is that \(P_1\) is the closest point to the intersection of the two planes with the 
function \(f\).

\subsection{The Tangent Vector}

Let \( \vec{X}(t) \in \Reals^n \) be a differentiable, parameterized curve. The \emph{tangent vector} 
at the point \( \vec{X}(t_0) \) is given by:

\[
    \vec{X}'(t_0) = \begin{bmatrix}
        x_1 ' (t_0) \\ x_2 ' (t_0) \\ \vdots \\ x_n ' (t_0)
    \end{bmatrix}
\]

This vector points in the direction in which the curve is moving at \( t_0 \), and its magnitude 
corresponds to the instantaneous speed. The unit tangent vector is:

\[
    \vec{X}(t) = \frac{\vec{X}'(t)}{\|\vec{X}'(t)\|}
\]

The speed of the vector in \(\Reals^2\) is given by:

\[
    \vec{X}(t) = \begin{bmatrix}
        t \\ f(t)
    \end{bmatrix}
\]

and the tangent is:

\[
    T(t) = \begin{bmatrix}
        t \\ f(t)
    \end{bmatrix} + \lambda \begin{bmatrix}
        1 \\ f'(t)
    \end{bmatrix}
\]

\subsubsection{Derivation of the Tangent Vector Formula}

Given a position vector function \(\vec{r}(t)\), the tangent vector to the curve described by 
\(\vec{r}(t)\) is obtained by taking the derivative of \(\vec{r}(t)\) with respect to time. This 
derivative is defined as the following limit:

\[
    \frac{d\vec{r}}{dt} = \lim_{\Delta t \to 0} \frac{\vec{r}(t + \Delta t) - \vec{r}(t)}{\Delta t}
\]

Assuming that \(\vec{r}(t)\) is expressed in terms of the canonical basis vectors \(\hat{\imath}, 
\hat{\jmath}, \hat{k}\), we can write:

\[
    \vec{r}(t) = x(t)\hat{\imath} + y(t)\hat{\jmath} + z(t)\hat{k}
\]

Then, the increment becomes:

\[
    \vec{r}(t + \Delta t) = x(t + \Delta t)\hat{\imath} + y(t + \Delta t)\hat{\jmath} + z(t + \Delta t)
    \hat{k}
\]

So the difference in the numerator of the derivative is:

\begin{align*}
    \vec{r}(t + \Delta t) - \vec{r}(t) &= \left[x(t + \Delta t) - x(t)\right]\hat{\imath} \\
    &\quad + \left[y(t + \Delta t) - y(t)\right]\hat{\jmath} \\
    &\quad + \left[z(t + \Delta t) - z(t)\right]\hat{k}
\end{align*}

Dividing by \(\Delta t\) and taking the limit:

\begin{align*}
    \frac{d\vec{r}}{dt} &= \lim_{\Delta t \to 0} \frac{\vec{r}(t + \Delta t) - \vec{r}(t)}{\Delta t} \\
    &= \left( \lim_{\Delta t \to 0} \frac{x(t + \Delta t) - x(t)}{\Delta t} \right) \hat{\imath} \\
    &\quad + \left( \lim_{\Delta t \to 0} \frac{y(t + \Delta t) - y(t)}{\Delta t} \right) \hat{\jmath} \\
    &\quad + \left( \lim_{\Delta t \to 0} \frac{z(t + \Delta t) - z(t)}{\Delta t} \right) \hat{k} \\
    &= \frac{dx}{dt} \hat{\imath} + \frac{dy}{dt} \hat{\jmath} + \frac{dz}{dt} \hat{k}
\end{align*}

This vector \(\frac{d\vec{r}}{dt}\) points in the direction of motion and is tangent to the curve at 
each point \(t\).




