\newpage
\section{Limits}

Limits are a foundational concept in calculus and analysis. They describe the behavior of 
functions and sequences as the input approaches a particular value or infinity.

\subsection{Definition of the Limit}

Let \(f\) be a function defined near a point \(a\). We say:

\[
    \lim_{x \to a} f(x) = L
\]

if for every \(\varepsilon > 0\), there exists a \(\delta > 0\) such that:

\[
    0 < |x - a| < \delta \Rightarrow |f(x) - L| < \varepsilon
\]

This is the precise \emph{\(\varepsilon\)-\(\delta\) definition} of a limit.

\subsection{Alternative Definition}

Given a neighborhood \(U\) of \(x_0\) then \(\dot{U}:= U \setminus \{x_0\} \) the \emph{pointed neighborhood} 
of \(x_0\). For \(\delta > 0\) we defined 

\[
    \dot{U_{\delta}}(x_0) = (x_0 - \delta, x_0 + \delta) \setminus \{x_0\}
\]

Using this we can then define the limit as: 

Given \(f: D \to \Reals\) with \(x_0 \in D\) being an accumulation point. \(L\) is the limit if 
for all \(\varepsilon > 0 \) there is a \(\delta > 0\) such that 

\[
    x \in D \cap \dot{U_{\delta}}(x_0) \text{ holds } |f(x) - L | < \varepsilon 
\]

\subsection{Limit Calculation Rules}

\emph{Sum Rule:} \( \lim_{x \to a} (f(x) + g(x)) = \lim_{x \to a} f(x) + \lim_{x \to a} g(x)\)

\emph{Difference Rule:} \( \lim_{x \to a} (f(x) - g(x)) = \lim_{x \to a} f(x) - \lim_{x \to a} g(x)\)

\emph{Product Rule:} \( \lim_{x \to a} (f(x)g(x)) = \lim_{x \to a} f(x) \cdot \lim_{x \to a} g(x)\)

\emph{Quotient Rule:} \(\lim_{x \to a} \frac{f(x)}{g(x)} = \frac{\lim f(x)}{\lim g(x)}\) if \(\lim g(x) \ne 0\)

\emph{Power Rule:} \( \lim_{x \to a} {f(x)}^n = {(\lim_{x \to a} f(x))}^n\)

\emph{Root Rule:} \( \lim_{x \to a} \sqrt[n]{f(x)} = \sqrt[n]{\lim_{x \to a} f(x)}\) (when defined)

\textbf{Proof of the Sum Rule:}

For any \(\varepsilon > 0, \text{ there exists a } \delta_1 > 0\) such that if \(0 < |x - a| < \delta\) then \(0 < |f(x) - L| < \varepsilon\). 
The same goes for \(g(x)\) with its respective limit and delta. Now choose \(\delta = \min\{\delta_1, \delta_2\}\) such that

\[
    |f(x) - L| < \frac{\varepsilon}{2}
\]

\[
    |g(x) - K| < \frac{\varepsilon}{2}
\]

This using now the triangle inequality we get 

\[
    |f(x) + g(x) - (L + K)| \le |f(x) - L|  + |g(x) - K| < 2\frac{\varepsilon}{2} = \varepsilon
\]

\QED

\subsection{Limits at Infinity}

\[
    \lim_{x \to \infty} f(x) = L \quad \text{means that } f(x) \text{ approaches } L \text{ as } x \to \infty
\]

\[
    \lim_{x \to \infty} \frac{1}{x} = 0, \quad \lim_{x \to \infty} e^x = \infty, \quad \lim_{x \to \infty} \frac{1}{x^2} = 0
\]

\subsection{Indeterminate Forms}

Indeterminate forms arise in limits when the expression does not directly imply a limit value:

\[
    \frac{0}{0}, \quad \frac{\infty}{\infty}, \quad 0 \cdot \infty, \quad \infty - \infty, \quad 1^\infty, \quad \infty^0, \quad 0^0
\]

\subsection{Limit of the n-th Root of n}

We will show that \(\lim_{n \to \infty} \sqrt[n]{n} = 1\)

\textbf{Proof:}

Let \(n = (1 + (\sqrt{n} -1))^n\). This is valid substitution. Then 

Now let use the binomial theorem \((1 + (\sqrt{n} -1))^n = (1 + b_n)^n\)  

\begin{align*}
    (1 + b_n)^n &= \sum_{k = 0}^{n} \binom{n}{k} b_n^k \\ 
     &= 1 + b_{n} + \frac{n(n - 1)}{2} b_{n}^{2} + \cdots\\ 
     &> \frac{n(n - 1)}{2} b_{n}^{2} 
\end{align*}

Then 

\begin{align*}
   (1 + b_n)^n = n > \frac{n(n - 1)}{2} b_{n}^{2} \\ 
   \sqrt{\frac{2}{n - 1}} > b_n
\end{align*}

Now if we take the limit 

\[
    \lim_{n \to \infty} \sqrt{\frac{2}{n - 1}} = 0 > \lim_{n \to \infty} b_n
\]

Then 

\[
    \lim_{n \to \infty} 1 + b_n = 1 + 0 = 1
\]

\QED 

\subsection{Limit of a Composite Function}

If \( \lim_{x \to a} f(x) = L\) and \( \lim_{x \to L} g(x) = g(L)\) 
(i.e., \(g\) is continuous at \(L\)), then:

\[
    \lim_{x \to a} g(f(x)) = g\left(\lim_{x \to a} f(x)\right)
\]

\subsection{Right and Left Limits}

For \((a - \delta, a )\) and \((a, a + \delta )\)

\[
    \lim_{x \to a^-} f(x) = L_-, \quad \lim_{x \to a^+} f(x) = L_+
\]

The two-sided limit \( \lim_{x \to a} f(x)\) exists if and only if \(L_- = L_+\).

\subsection{L’Hôpital’s Rule}

If \( \lim_{x \to a} f(x) = \lim_{x \to a} g(x) = 0\) or \(\pm\infty\), and \( \lim_{x \to a} \frac{f'(x)}{g'(x)}\) exists, then:

\[
    \lim_{x \to a} \frac{f(x)}{g(x)} = \lim_{x \to a} \frac{f'(x)}{g'(x)}
\]

This rule is used to resolve indeterminate forms \(\frac{0}{0}\) and \(\frac{\infty}{\infty}\).

\subsection{Limit of a fraction}

For expressions like:

\[
    \lim_{n \to \infty} {\left( \frac{a}{b} \right)}^n
\]

\begin{itemize}

    \item If \(\frac{a}{b} < 1\): limit is \(0\)

    \item If \(\frac{a}{b} > 1\): limit diverges to \(\infty\)

\end{itemize}

\subsection{Limit due to Sequence}

Given an accumulation point \(x_0\) in the domain of \(f\) then 

\[
    \lim_{x \to x_0} f(x) = L
\]

If and only if for the sequence in the domain of \(f \) the following is true

\[
    \lim_{n \to \infty} x_n = x_0
\]

\textbf{Proof:}

\(\Rightarrow\) Assume \(\lim_{x \to x_0}f(x) = L\). Let \(a_n\) be a sequence in the domain with \(a_n \to x_0\) and 
\(a_n \ne x_0\) we must show that \(f(a_n) \to L\)

Let \(\varepsilon > 0\). There exists \(\delta > 0\) such that if \(0 < |x - x_0| < \delta\) then 
\(|f(x) - L| < \varepsilon\). But since \(a_n \to x_0\) then \(\exists N: |a_n - x_0| < \varepsilon, \forall n > N \). Thus, 
for all \(n > N\), \(|f(a_n) - L| < \varepsilon\) and so \(f(a_n) \to L\). 

\(\Leftarrow\) Assume that for every sequence \(a_n\) in the domain which converges to \(x_0\)
\(f(a_n) \to L\)

Suppose for the sake of contradiction that \(\lim_{x \to x_0}f(x) \ne L\)

Then, there exists \(\varepsilon > 0\) such that for all \(\delta > 0, \exists x \in \mathcal{D}, 0 < |x - x_0| < \delta\)
where \(|f(a_n) - L| \ge \varepsilon\). Therefore, by letting \(\delta_n = \frac{1}{n}\), 
\(\exists a_n \in \mathcal{D}\) satisfiying \(0 < |a_n - x_0| < \frac{1}{n}\) and which 
\(|f(a_n)- L| \ge \varepsilon\). 

Then by contradiction \(f(a_n)\) does not converge to \(L\). 

Hence, it must be that \(\lim_{x \to x_0}f(x) \ne L\)

\QED

\subsection{Limit of a Recursive Sequence}

Let \((a_n)\) be defined recursively:

\[
    a_{n+1} = f(a_n)
\]

If \((a_n)\) converges to \(L\) and \(f\) is continuous, then:

\[
    \lim_{n \to \infty} a_n = L \Rightarrow L = f(L)
\]

This is used to find fixed points of recursive definitions.

\subsection{Important Limits}

\begin{itemize}

    \item \( \lim_{x \to 0} \frac{\sin x}{x} = 1\)

    \item \( \lim_{x \to 0} \frac{1 - \cos x}{x^2} = \frac{1}{2}\)

    \item \( \lim_{x \to 0} \frac{\ln(1 + x)}{x} = 1\)

    \item \( \lim_{x \to 0} \frac{e^x - 1}{x} = 1\)

    \item \( \lim_{n \to \infty} \frac{n!}{n^n} = 0\)

    \item \( \lim_{n \to \infty} \sqrt[n]{a} = 1\) (for \(a > 0\))

    \item \( \lim_{x \to 0} \frac{1}{\cos x} = 1\)

\end{itemize}

\subsection{Squeeze Theorem (Sandwich Theorem)}

If \(g(x) \le f(x) \le h(x)\) near \(a\), and:

\[
    \lim_{x \to a} g(x) = \lim_{x \to a} h(x) = L
    \Rightarrow \lim_{x \to a} f(x) = L
\]

\textbf{Proof:}

We know that \(g(x) \le f(x) \le h(x)\) therefore, for \(g(x)\) and \(h(x)\) we 
can define the following inequalities. 

\begin{align*}
    |g(x)_{N_1} - L| &< \varepsilon \\ 
    |h(x)_{N_2} - L| &< \varepsilon 
\end{align*}

Then let us define \(n = \operatorname{max}(N_1, N_2)\). And we can also write 

\begin{align*}
    - \varepsilon &< g(x)_{n} - L < \varepsilon \\ 
    - \varepsilon &< h(x)_{n} - L < \varepsilon 
\end{align*}

Now, we write the following because we are assuming they all have the same limit 

\begin{align*}
    - \varepsilon &< g(x)_{n} - L \le f(x)_n - L \le h(x)_{n} - L < \varepsilon \\ 
    - \varepsilon &<  f(x)_n - L < \varepsilon 
\end{align*}

from our original inequality. Thus, there has been no contradiction.

\QED

\textbf{Example:}

\[
    \lim_{x \to 0} x^2 \cos\left( \frac{1}{x} \right) = 0
\]

\subsection{The Number \texorpdfstring{\(e\)}{e} as a Limit}

The number \(e\) is defined as:

\[
    e = \lim_{n \to \infty} {\left(1 + \frac{1}{n} \right)}^n
\]

More generally:

\[
    \lim_{n \to \infty} {\left(1 + \frac{a}{b n^d}\right)}^{n^d} = e^{\frac{a}{b}}
\]


\textbf{Example I:}

\[
    \lim_{x \to \infty}{\left(\frac{x^2 + 1}{x^2 +2}\right)}^{x^2}
\]

\[
    \lim_{x \to \infty} {\left({\left(\frac{x^2 + 1}{x^2 +2}\right)}^{x}\right)}^x
\]

\[
    \lim_{x \to \infty} {\left({\left(\frac{1 + \frac{1}{x^2}}{1 + \frac{2}{x^2}}\right)}^{x}\right)}^x
\]

\[
    \lim_{x \to \infty} {\left(\frac{e}{e^2}\right)}^x = {\left(\frac{1}{e}\right)}^x = 0
\]

\textbf{Example II:}

\[
    \lim_{x \to \infty}\left( 1 + \frac{1}{x}\right)^{x^2}
\]

\[
    \lim_{x \to \infty} \left(\left( 1 + \frac{1}{x}\right)^{x^2}\right)^\frac{x}{x}
\]

\[
    \lim_{x \to \infty} \left(\left( 1 + \frac{1}{x}\right)^{x}\right)^x
\]

\[
    \lim_{x \to \infty} \left(\lim_{n \to \infty} \left( 1 + \frac{1}{x}\right)^{x}\right)^x
\]


\[
    \lim_{x \to \infty} \left( e \right)^x = \infty
\]

\subsection{Limits of \texorpdfstring{\(\arctan\)}{arctan}}

\begin{itemize}

    \item \(\lim_{x \to \infty} \arctan x = \frac{\pi}{2}\)

    \item \(\lim_{x \to -\infty} \arctan x = -\frac{\pi}{2}\)

    \item \(\arctan x\) is continuous and differentiable everywhere, with a horizontal asymptote at 
    \(y = \pm \frac{\pi}{2}\)

\end{itemize}

\subsection{Bolzano-Weierstraß Theorem}

Every bounded, convergent sequence in \(\Complex\) has a convergent subsequence.
More precisely the sequence \(a\) has biggest and smallest accumulation point \(h^*\) and \(h_*\)

\[
    \forall \varepsilon > 0 : h_* - \varepsilon < a_n < h^{*} + \varepsilon \text{ for all-most all } 
    b \in \Naturals
\]

\[
    h^* := \lim_{n \to \infty} \sup a_n
\]

\[
    h_* := \lim_{n \to \infty} \inf a_n
\]

\subsection{Uniqueness of the Limit}

Let \( f: D \subset \Reals \to \Reals \), and suppose \(a\) is a limit point of \( D \). Then:
If \( \lim_{x \to a} f(x) = L_1 \) and \( \lim_{x \to a} f(x) = L_2 \), then \( L_1 = L_2 \).

In other words, a function can have at most one limit at a given point. This is the \emph{uniqueness of limits}.

\textbf{Proof:}

Assume \( L_1 \ne L_2 \), and let \( \varepsilon = \frac{|L_1 - L_2|}{2} > 0 \).

\begin{itemize}

    \item Since \( \lim_{x \to a} f(x) = L_1 \), there exists \( \delta_1 > 0 \) such that for all \( x \in D \), \( 0 < |x - a| < \delta_1 \) implies:

    \[
        |f(x) - L_1| < \varepsilon
    \]
    
    \item Since \( \lim_{x \to a} f(x) = L_2 \), there exists \( \delta_2 > 0 \) such that for all \( x \in D \), \( 0 < |x - a| < \delta_2 \) implies:

    \[
        |f(x) - L_2| < \varepsilon
    \]

\end{itemize}

Let \( \delta = \min(\delta_1, \delta_2) \), and choose \( x \in D \) such that \( 0 < |x - a| < \delta \).

Then:

\[
    |f(x) - L_1| < \varepsilon, \quad |f(x) - L_2| < \varepsilon
\]

Using the triangle inequality:

\[
    |L_1 - L_2| = |L_1 - f(x) + f(x) - L_2| \le |f(x) - L_1| + |f(x) - L_2| < \varepsilon + \varepsilon = 2\varepsilon
\]

Thus, 

\[
    |L_1 - L_2|  <  |L_1 - L2|
\]


This contradiction tells us that, our assumption was false. Hence, \( L_1 = L_2 \). The limit is unique.

\QED

\subsection{Every Convergent Sequence is a Cauchy Sequence}

Let \( (a_n) \) be a sequence in \( \Reals \). We say:

If \( (a_n) \) converges, then it is a Cauchy sequence.

A sequence \( (a_n) \) is a \emph{Cauchy sequence} if:

\[
    \forall \varepsilon > 0, \ \exists N \in \Naturals \text{ such that } \forall n, m \ge N, \quad |a_n - a_m| < \varepsilon
\]

\textbf{Proof:}

Assume \( \lim_{n \to \infty} a_n = L \). Then:

\begin{itemize}

    \item For every \( \varepsilon > 0 \), there exists \( N \in \Naturals \) such that:

    \[
        \forall n \ge N, \quad |a_n - L| < \frac{\varepsilon}{2}
    \]

    \item Now, for any \( n, m \ge N \), use the triangle inequality:

    \[
        |a_n - a_m| = |a_n - L + L - a_m| \le |a_n - L| + |a_m - L| < \frac{\varepsilon}{2} + \frac{\varepsilon}{2} = \varepsilon
    \]

\end{itemize}

\textbf{Conclusion:}  

\(|a_n - a_m| < \varepsilon \text{ for all } n, m \ge N \Rightarrow (a_n) \text{ is Cauchy}\)

This completes the proof that every convergent sequence is Cauchy.

\QED

\subsection{Limsup and Liminf}

Given a sequence \(\{a_k\}_{k = 1}^{\infty}\) and the infinite sequences \(x_n\) and \(y_n\) defined as 

\[
    x_n := \sup {a_k \mid k \ge n} \quad y_n := \inf {_k \mid k \ge n}
\]

We define \emph{limes superior} and \emph{limes inferior} as 

\[
    a^* = \lim_{n \to \infty} \sup a_n := \lim_{n \to \infty} x_n 
\]
\[
    a_* = \lim_{n \to \infty} \inf a_n := \lim_{n \to \infty} y_n 
\]

\subsection{Limit by Bounds}

If the limit superior and inferior are equal then the limit exists.