\newpage
\section{Linear Maps}

A linear map \( f: V \to W \) is a function that satisfies the following properties:

\begin{itemize}
    
    \item \( f(v_1 + v_2) = f(v_1) + f(v_2) \) for all \( v_1, v_2 \in V \).
    
    \item \( f(\lambda v) = \lambda f(v) \) for all \( v \in V \) and \( \lambda \in \Reals \).
    
    \item \( f(0) = 0 \).

\end{itemize} 

This kind of function is also called \emph{Homomorphism}. If \(V = W\) is it called \emph{Endomorphism}.

\subsection{Types of Linear Maps}

\begin{itemize}
    
    \item \emph{Injective or Monomorphism} (One-to-One): A linear map \( f: V \to W \) is injective if 
          \( f(v_1) = f(v_2) \) implies \( v_1 = v_2 \).
    
    \item \emph{Surjective or Epimorphism} (Onto): A linear map \( f: V \to W \) is surjective if for 
          every \( w \in W \), there exists a \( v \in V \) such that \( f(v) = w \).
    
    \item \emph{Bijective or Isomorphism}: A linear map \( f: V \to W \) is bijective if it is both 
          injective and surjective.

\end{itemize}

\emph{Isomorphic} if there exists a bijective linear map between them. In this case, we can say 
that the two vector spaces are \emph{isomorphic}, 
and we write \( V \cong W \).

\subsection{Properties of Linear Maps}

\begin{itemize}
    
    \item The composition of two linear maps is a linear map.
    
    \item The inverse of a bijective linear map is also a linear map.
    
    \item The zero map \( f: V \to W \) defined by \( f(v) = 0 \) 
          for all \( v \in V \) is a linear map.
    
    \item The identity map \( \text{id}_V: V \to V \) defined by \( \text{id}_V(v) = v \) for all 
          \( v \in V \) is a linear map.
    
    \item The sum of two linear maps \( f: V \to W \) 
          and \( g: V \to W \) is a linear map defined by \( (f + g)(v) = f(v) + g(v) \).
    
    \item The scalar multiplication of a linear map \( f: V \to W \) by a scalar \( c \) is a linear 
          map defined by \( (cf)(v) = c(f(v)) \).
    
    \item The composition of linear maps is associative, i.e., \( (f \circ g) \circ h = f \circ (g \circ h) \).
    
    \item The composition of linear maps is distributive over addition, i.e., 
          \( f \circ (g + h) = f \circ g + f \circ h \).
    
    \item The composition of linear maps is compatible with scalar multiplication, i.e., 
          \( (cf) \circ g = c(f \circ g) \).
    
    \item Linear Maps compose a vector space 
          over the field of scalars.
          
          \[
            (Hom(V,W), +, \cdot )
          \]

\end{itemize}

\subsection{Composition as Matrix Multiplication}

Given \(A \in \Field^{m \times n}, B \in \Field^{n \times l}\) and  \(\phi_A: \Field^{n \times 1} \to \Field^{m \times 1}\) and 
\(\phi_B: \Field^{l \times 1} \to \Field^{n \times 1}\) induced linear maps then 

\begin{itemize}

    \item
    \[
        \phi_A \circ \phi_B \text{ is a linear map}
    \]

    \item 
    \[
        \phi_A \circ \phi_B = \phi_{AB}
    \]

\end{itemize}

\textbf{Proof I:}

\begin{align*}
(\varphi_A \circ \varphi_B)(au + bv) &= \varphi_A(\varphi_B(au + bv))\\ 
&= \varphi_A(a\,\varphi_B(u) + b\,\varphi_B(v))\\ 
&= a\,\varphi_A(\varphi_B(u)) + b\,\varphi_A(\varphi_B(v)) \\ 
&= a(\varphi_A \circ \varphi_B)(u) + b(\varphi_A \circ \varphi_B)(v) 
\end{align*}

\QED 

\textbf{Proof II:}

\begin{align*}
    (\varphi_A \circ \varphi_B)(x) &= \varphi_A(\varphi_B(x)) \\
    &= \varphi_A(Bx) \\
    &= A(Bx) \\
    &= (AB)x \\
    &= \varphi_{AB}(x)
\end{align*}

\QED

\subsection{Injectivity and Surjectivity}

Let \( A \in \Field^{m \times n} \). Then:

\( \varphi_A \) is injective if and only if the fiber of
\( \varphi_A \) over the zero column \( 0_m \in \Field^{m \times 1} \)
consists only of the zero column \( 0_n \in \Field^{n \times 1} \).

\( \varphi_A \) is surjective if and only if every column in
\( \Field^{m \times 1} \) can be written as a linear combination of
the columns of \( A \).

\subsection{Linear Extension Theorem}

Let \(\beta = \{v_1, \dots, v_n\}\) be a basis of the finite dimensional vector space \(V\) and 
\(w_1, \dots, w_n\) be arbitrary vector in \(W\). Then \(!\exists L\) with \(L : V \to W\) such 
that \(T(v_i) = w_i\).

\textbf{Proof:}

Define \(T\) as follows: Whenever \(a = \lambda_1 v_1 + \cdots + \lambda_n v_n\) then 
\(T(a) = \lambda_1 w_1 + \cdots + \lambda_n w_n\).

We know it is unique because for
\(\lambda_1, \dots, \lambda_n \ne \gamma_1, \dots, \gamma_n\) if we assume \(a - b = \vec{0}\) then:

\[
    a - b = (\lambda_1  - \gamma_1) v_1 + \cdots + (\lambda_n - \gamma_n) v_n = \vec{0}
\]

If and only if \(\lambda_1, \dots, \lambda_n = \gamma_1, \dots, \gamma_n\). Thus, it works uniquely 
for each vector.
    
For the linearity, using the definition of linearity and \(T\) this part becomes trivial.

Finally, for the uniqueness of \(T\) itself, let us assume there is a map \(U\) which 
\(U(v_i) = T(v_i)\). Because we can write a vector a linear combination we will arrive at the 
fact that 

\[
    U(x) = \lambda_1U(v_1) + \dots + \lambda_n U(v_n) =  \lambda_1Tv_1) + \dots + \lambda_n T(v_n) 
\]

Therefore, \(U = T\).

\QED

\subsection{The Kernel of a Linear Map}

The kernel of a linear map \( f: V \to W \) is the set of all vectors in \( V \) that are mapped to 
the zero vector in \( W \):

\[
    \ker(f) = \{ v \in V \mid f(v) = 0 \}
\]

To compute the \emph{kernel} of the linear map, we just solve the homogenous system of equations \(Ax = \vec{0}\) 
where \(A\) is the matrix representation of \(f\). The number of free variables is the \(\dim(\ker(f))\).

\subsection{The Image of a Linear Map}

The image of a linear map \( f: V \to W \) is the set of all vectors in \( W \) that can be expressed 
as \( f(v) \) for some \( v \in V \):

\[
    \img(f) = \{ w \in W \mid w = f(v) \text{ for some } v \in V \}
\]

To find \( \dim(\text{im}(f)) \), compute the rank of the matrix representing \( f \). This 
is the number of pivot columns in the row-reduced form of \( A \).

\subsection{The Rank of a Linear Map}

The rank of a linear map \( f: V \to W \) is the dimension of its image:

\[
    \rank(f) = \img(f)
\]

\subsection{The Nullity of a Linear Map}

The nullity of a linear map \( f: V \to W \) is the dimension of its kernel:
    
\[
    \text{nullity}(f) = \ker(f)
\]

\subsection{Proof of the injectivity of the Kernel}

Suppose we have a linear map \(f\) and two vectors \(v_1\) and \(v_2\) which are not equal. 
We are going to assume that they both map to the \(\vec{0}\) therefore:

\[
    f(v_1) = f(v_2) = \vec{0}
\]

Because of the linearity of the map we can write:

\[
    f(v_1 - v_2) = f(v_1) - f(v_2) = \vec{0} - \vec{0} = \vec{0}
\]

And that contradicts the assumption that \(v_1\) and \(v_2\) are not equal. Therefore, the 
kernel of a linear map is injective.

\subsection{Dimension Formula for Linear Mappings}

Let \(f: V \to W\) be linear and \(\dim(V) = n\). Then
    
\[
    \dim(\ker(f)) + \rank(f) = n.
\]
    
Remember that \(\ker(f)\) forms a subspace of \(V\) and therefore, \(\dim(\ker(f)) := r \leq n\). We extend an arbitrary basis \((v_1, \ldots, v_r)\) of \(\ker(f)\) to a basis \((v_1, \ldots, v_r, v_{r+1}, \ldots, v_n)\) of \(V\). Setting \(w_{r+i} = f(v_{r+i})\) for \(i = 1, \ldots, n - r\), we have \(\forall v \in V\):

\begin{align*}
    f(v) &= f(\lambda_1v_1 + \cdots + \lambda_r v_r + \lambda_{r+1}v_{r+1} + \cdots + \lambda_n v_n) \\
    &= \lambda_1 \underbrace{f(v_1)}_{=0} + \cdots + \lambda_r \underbrace{f(v_r)}_{=0} + \lambda_{r+1} f(v_{r+1}) + \cdots + \lambda_n f(v_n) \\
    &= \lambda_{r+1} f(v_{r+1}) + \cdots + \lambda_n f(v_n) \\
    &= \lambda_{r+1}w_{r+1} + \cdots + \lambda_n w_n
\end{align*}
    
Thus, \(\img(f) = \operatorname{span}(w_{r+1}, \ldots, w_n)\). We now show that \(w_{r+1}, \ldots, w_n\) are linearly independent. Let 
    
\[
    \lambda_{r+1}w_{r+1} + \cdots + \lambda_n w_n = 0
\]
    
From

\[
    0 = \lambda_{r+1}w_{r+1} + \cdots + \lambda_n w_n = f(\lambda_{r+1}v_{r+1} + \cdots + \lambda_n v_n)
\]
    
it follows that

\[
    \lambda_{r+1}v_{r+1} + \cdots + \lambda_n v_n \in \ker(f)
\]
    
Therefore,
    
\[
    \lambda_{r+1}v_{r+1} + \cdots + \lambda_n v_n = \lambda_1v_1 + \cdots + \lambda_r v_r
\]\

For some \(\lambda_1, \ldots, \lambda_r\). Since \(v_1, \ldots, v_n\) are linearly independent, 
we have \(\lambda_1 = \cdots = \lambda_n = 0\). Thus, the vectors \(w_{r+1}, \ldots, w_n\) are linearly 
independent.
    
It follows that \(\dim(\img(f)) = \rank(f) = n - r\) and therefore,
    
\[
    \dim(\ker(f)) + \dim(\img(f)) = \dim(\ker(f)) + \rank(f) = r + n - r = n
\]

\QED
  
\subsection{Linear Maps and Matrices}

Let \( V \) and \( W \) be finite-dimensional vector spaces over the same field \( \Field \). 
If \( \dim V = n \) and \( \dim W = m \), then a linear map \( f: V \to W \) can be represented by 
an \( m \times n \) matrix. The action of the linear map on a vector can be expressed as:

\[
    f(x) = A \cdot x,
\]

where \(A\) is the matrix representation of \(f\) and \( v \) is the vector represented in a column 
format. The columns of the matrix \(A\) are the images of the basis vectors of \( V \) under the linear 
map \(f\).

This fact comes from \(f\) being linear which means that 

\[
    f(x) = x_1f(e_1) + \cdots + x_nf(e_n)
\]

with \(e_i\) being a basis vector. And from that expression we can derive \((f(e_1), \dots, f(e_n))x\).

We can state that matrices \emph{induce} linear maps which we are denoted as \(\varphi_A\).

\subsection{How to identify the type of Linear Map Summary}

We want to summarize how to identify the type of linear map given \( f: V \to W \),

\begin{itemize}

    \item \emph{Injective}: A linear map \( f \) is injective if and only if

    \[
        \dim(\ker(f)) = 0.
    \]

    \item \emph{Surjective}: The map \( f \) is surjective if every element of \( W \) is the 
    image of some element in \( V \). This occurs when

    \[
        \dim(\img(f)) = \dim(W).
    \]

    That is, the image of \( f \) are the linear independent vector of \(f\).

    \item \emph{Bijective}: A linear map is bijective if it is both injective and surjective. 
    Therefore, it must satisfy both conditions:
    
    \[
        \dim(\ker(f)) = 0 \quad \text{and} \quad \dim(\text{im}(f)) = \dim(W).
    \]
    
    \item \emph{Neither}: If neither of the above conditions is satisfied, the map is neither injective 
    nor surjective.

\end{itemize}

\textbf{Example:}

Let \( f: \Reals^3 \to \Reals^2 \) be defined by the matrix

\[
    A = \begin{bmatrix} 2 & 1 & 1 \\ 2 & 3 & 2 \end{bmatrix}.
\]

To identify the type of this linear map:

\begin{itemize}
    
    \item Compute \( \dim(\ker(f)) \): By reducing the matrix \( A \) to row echelon form, we find that 
    \( \dim(\ker(f)) = 1 \). This means \( f \) is not injective.
    
    \item Compute \( \dim(\text{im}(f)) \): The image corresponds to the column space of \( A \). Since 
    there are 2 linearly independent rows in the row echelon form, we have \( \dim(\text{im}(f)) = 2 \), 
    which equals \( \dim(W) = 2 \). Therefore, \( f \) is surjective.

\end{itemize}

Thus, \( f \) is surjective but not injective.


