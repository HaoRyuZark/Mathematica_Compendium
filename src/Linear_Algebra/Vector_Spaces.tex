\newpage
\section{Vector Spaces}

Given a field \(\Field\). A \emph{vector space} is a set \((V, \Field, +, \circ)\) with two operations, vector addition and scalar multiplication, 
such that:

\begin{enumerate}[label=\Roman*.]

	\item \((a + b)\vec{v} = a\vec{v} + b\vec{v}, \forall a,b \in \Field land \vec{v} \in V\)
	
	\item \((\vec{v} + \vec{u})a = a\vec{v} + a\vec{u}, \forall a\in \Field land \vec{v}, \vec{u} \in V\)

	\item \((ab)\vec{v} = a(b\vec{v}), \forall a,b \in \Field land \vec{v} \in V\)

	\item \(1 \vec{v} = \vec{v}, \forall \vec{v} \in V\)

\end{enumerate}

The scalar multiplication is defined as  

\[
	\Field \times  V \to V: (a,v) \mapsto av
\]

Also using these axioms, we get 

\begin{itemize}

	\item \(\vec{v}0 = \vec{0}\)

	\item \((-1)\vec{v} = -1\vec{v}\)

\end{itemize}

\textbf{Proof:} 

For the first property, \(0\vec{v} = (0 + 0)\vec{v} = 0\vec{v} + 0\vec{v}\) then 

\[
	0\vec{v} = 0\vec{v} + 0\vec{v} \implies \vec{0} = 0\vec{v} 
\]

For the second property, 

\begin{align*}
	(-1)\vec{v} &= (1 - 2)\vec{v} \\ 
	(-1)\vec{v} &= \vec{v} - 2\vec{v} \\  
	(-1)\vec{v} &= -\vec{v}  
\end{align*}

\QED

\textbf{Examples:}

\begin{enumerate}

	\item The set of all \(n\)-tuples of real numbers \(\Reals^n\) is a vector space over the field 
	      of real numbers \(\Reals\).

	\item The set of all polynomials of degree less than or equal to \(n\) is a vector space over the 
	      field of real numbers \(\Reals\).

	\item The set of all continuous functions from \(\Reals\) to \(\Reals\) is a vector space 7
	      over the field of real numbers \(\Reals\).

	\item The set of all \(m \times n\) matrices with real entries is a vector space over the field of 
	      real numbers \(\Reals\).

	\item The set of all linear maps is also a vector space. 

\end{enumerate}

\subsection{Subspaces}

A subset \(W\) of a vector space \(V\) is a \emph{subspace} of \(V\) if:

\begin{enumerate}[label=\Roman*.]

	\item The zero vector \(\vec{0} \in W\).

	\item For all \(\vec{u}, \vec{v} \in W\), \(\vec{u} + \vec{v} \in W\).

	\item For all \(a \in F\) and \(\vec{v} \in W\), \(a\vec{v} \in W\).

\end{enumerate}

If \(W\) is a subspace of \(V\), we write \(W \subseteq V\).

The intersection of two subspaces is also a subspace.

\subsection{Linear Combinations}

A \emph{linear combination} of vectors \(\vec{v}_1, \vec{v}_2, \ldots, \vec{v}_n\) in a vector space 
\(V\) is an expression of the form:

\[
	a_1\vec{v}_1 + a_2\vec{v}_2 + \cdots + a_n\vec{v}_n
\]

Where \(a_1, a_2, \ldots, a_n\) are scalars from the field \(F\).
The set of all linear combinations of a set of vectors \(\{\vec{v}_1, \vec{v}_2, \ldots, \vec{v}_n\}\) 
is called the \textbf{span} of those vectors, 
denoted by \(\text{span}(\vec{v}_1, \vec{v}_2, \ldots, \vec{v}_n)\). The span of a set of vectors is a 
subspace of the vector space \(V\).

The span of a set of vectors is the smallest subspace containing those vectors.

\subsection{Properties of the subspaces}

\begin{itemize}

	\item The intersection of two subspaces is a subspace.

	\item The union of two subspaces is not necessarily a subspace.

	\item The sum of two subspaces \(U\) and \(W\) is defined as:
	      \[
		      U + W = \{\vec{u} + \vec{w} : \vec{u} \in U, \vec{w} \in W\}
	      \]
	      The sum of two subspaces is a subspace.

		\end{itemize}

The sum of two subspaces is the smallest subspace containing both subspaces.

\begin{itemize}

	\item The direct sum of two subspaces \(U\) and \(W\) is defined as:
	      \[
		      U \oplus W = \{\vec{u} + \vec{w} : \vec{u} \in U, \vec{w} \in W\}
	      \]
	      The direct sum of two subspaces is a subspace.

	\item The direct sum of two subspaces is the smallest subspace containing both subspaces, such that 
		  \(U \cap W = \{\vec{0}\}\).

	\item The direct sum of two subspaces is denoted by \(U \oplus W\).

\end{itemize}

\subsection{Linear Independence}

A set of vectors \(\{\vec{v}_1, \vec{v}_2, \dots, \vec{v}_n\}\) in a vector space \(V\) is said to 
be \emph{linearly independent} if the only solution to the equation:

\[
	\lambda_1\vec{v}_1 + \lambda_2\vec{v}_2 + \cdots + \lambda_n\vec{v}_n = 0
\]

or

\[
	\sum_{i=1}^n \lambda_i \vec{v}_i = 0
\]

is \(a_1 = a_2 = \cdots = a_n = 0\). If there exists a non-trivial solution to this equation, 
then the set of vectors is said to be linearly dependent. A set of vectors is linearly 
independent if and only if the only linear combination of those vectors that equals the zero vector 
is the trivial combination where all coefficients are zero.

\subsubsection{Properties of the linear independence}

\begin{itemize}

	\item A set of vectors \(\{\vec{v}_1, \vec{v}_2, \ldots, \vec{v}_n\}\) is linearly independent 
	      if and only if the only linear combination of those vectors that equals the zero vector is 
		  the trivial combination where all coefficients are zero.

	\item If a set of vectors \(\{\vec{v}_1, \vec{v}_2, \ldots, \vec{v}_n\}\) is linearly independent, 
	      then any subset of that set is also linearly independent.

	\item If a set of vectors \(\{\vec{v}_1, \vec{v}_2, \ldots, \vec{v}_n\}\) is linearly dependent, 
	      then at least one vector in that set can be expressed as a linear combination of the others.

\end{itemize}

\subsection{Base}

\begin{itemize}

	\item A \emph{base} of a vector space \(V\) is a set of vectors 
	      \(\{\vec{v}_1, \vec{v}_2, \ldots, \vec{v}_n\}\) that is linearly independent and 
		  spans the vector space \(V\).

	\item The number of vectors in a base of a vector space is called the \emph{dimension} of the 
	      vector space.

	\item The dimension of a vector space \(V\) is denoted by \(\dim(V)\).

	\item If \(V\) has a finite base, then it is said to be \emph{finite-dimensional}.

	\item If \(V\) does not have a finite base, then it is said to be \emph{infinite-dimensional}.
\end{itemize}

\subsection{Dimension}

The \emph{dimension} of a vector space \(V\) is the number of vectors in a base of \(V\).

\begin{itemize}

	\item The dimension of a vector space is denoted by \(\dim(V)\).

	\item The dimension of a vector space can be finite or infinite.

	\item If the dimension of a vector space is finite, then it is said to be \emph{finite-dimensional}.

	\item If the dimension of a vector space is infinite, then it is said to be \emph{infinite-dimensional}.

\end{itemize}

\subsubsection{How to find the base of a set vector}

To find the base of a set of vectors, we can use the following steps:

\begin{enumerate}

	\item Write the vectors as columns of a matrix.

	\item Row-reduce the matrix to echelon form.

	\item The non-zero rows of the echelon form matrix correspond to the base of the vector space 
	      spanned by the original set of vectors.

\end{enumerate}

The number of non-zero rows in the echelon form matrix is equal to the dimension of the vector space 
spanned by the original set of vectors.

The base of a vector space is not unique. Different bases can span the same vector space.

\subsection{Basis Extension Theorem}

Let \(V\) be a vector space over a field \(K\), and let 

\[
	v_1, \ldots, v_r,\quad w_1, \ldots, w_s \in V.
\]

Suppose that \((v_1, \ldots, v_r)\) is a linearly independent tuple and that

\[
	\text{span}(v_1, \ldots, v_r, w_1, \ldots, w_s) = V.
\]

Then it is possible to extend \((v_1, \ldots, v_r)\) to a basis of \(V\) by possibly adding suitable 
vectors from the set \(\{w_1, \ldots, w_s\}\).

\textbf{Proof:}

If \(\text{span}(v_1, \ldots, v_r) = V\), the statement is obvious. So assume

\[
	\text{span}(v_1, \ldots, v_r) \neq V.
\]

Then there exists at least one \(w_i\) such that \(w_i \notin \text{span}(v_1, \ldots, v_r)\); 
otherwise, if all \(w_i \in \text{span}(v_1, \ldots, v_r)\), then

\[
	\text{span}(v_1, \ldots, v_r, w_1, \ldots, w_s) = \text{span}(v_1, \ldots, v_r) = V,
\]
which contradicts our assumption that \(\text{span}(v_1, \ldots, v_r) \neq V\).

The tuple \((w_i, v_1, \ldots, v_r)\) is linearly independent, because of

\[
	\sum_{j=1}^r \lambda_j v_j + \lambda w_i = 0
\]

It follows that \(\lambda = 0\) (since \(w_i \notin \text{span}(v_1, \ldots, v_r)\)), and then also 
\(\lambda_j = 0\) for all \(j\) because the \(v_j\) are linearly independent.

Possibly, \((w_i, v_1, \ldots, v_r)\) is still not a basis of \(V\). Then we repeat the previous step and 
keep adding further \(w_i\) until the tuple extends \((v_1, \ldots, v_r)\) to a basis of \(V\). This 
process terminates after finitely many steps, since

\[
	\text{span}(v_1, \ldots, v_r, w_1, \ldots, w_s) = V.
\]

\QED

Every finitely generated vector space \(V\) has a basis.

\subsection{Exchange Lemma}

Let \((v_1, \ldots, v_n)\) and \((w_1, \ldots, w_m)\) be bases of a vector space \(V\). Then, for every 
\(v_i\), there exists a \(w_j\) such that if we replace \(v_i\) by \(w_j\) in the tuple 
\((v_1, \ldots, v_n)\), it still forms a basis of \(V\).

\textbf{Proof:}

Let \((v_1, \ldots, v_n)\) and \((w_1, \ldots, w_m)\) be two bases of \(V\). Suppose we remove \(v_i\) 
from the first basis. The truncated tuple \((v_1, \ldots, v_{i-1}, v_{i+1}, \ldots, v_n)\) satisfies

\[
	\text{span}(v_1, \ldots, v_{i-1}, v_{i+1}, \ldots, v_n) \neq V,
\]

because if \(\text{span}(v_1, \ldots, v_{i-1}, v_{i+1}, \ldots, v_n) = V\), then \(v_i\) would lie in the 
span of the remaining vectors and could be written as a linear combination of them. This would contradict 
the assumption that \((v_1, \ldots, v_n)\) is linearly independent and a basis of \(V\).

By the Basis Extension Theorem, we can extend the truncated 
tuple \((v_1, \ldots, v_{i-1}, v_{i+1}, \ldots, v_n)\) to a basis of \(V\) by 
adding vectors from \((v_1, \ldots, v_{i-1}, v_{i+1}, \ldots, v_n, w_1, \ldots, w_m)\). 
Therefore, by the Basis Extension Theorem, there exists a \(w_j\) such that

\[
	w_j \notin \text{span}(v_1, \ldots, v_{i-1}, v_{i+1}, \ldots, v_n),
\]

and the tuple \((v_1, \ldots, v_{i-1}, v_{i+1}, \ldots, v_n, w_j)\) is linearly independent.

If this tuple does not form a basis, we can again apply the Basis Extension Theorem 
and add one of the vectors \(v_1, \ldots, v_n\) to complete the basis. Clearly, the only possibility is 
to add \(v_i\), but this would imply that the tuple \((v_1, \ldots, v_n, w_j)\) is not a basis, as 
\(w_j\) would then be linearly dependent on the other vectors. 
Therefore, \((v_1, \ldots, v_{i-1}, v_{i+1}, \ldots, v_n, w_j)\) must form a basis of \(V\).

\QED

\subsection{Dimension of a sum of subspaces}

Let \(U\) and \(W\) be two subspaces of a vector space \(V\). Then the dimension of the sum of the two 
subspaces is given by:

\[
    \dim(U + W) = \dim(U) + \dim(W) - \dim(U \cap W)    
\]

\subsection{Linear Independence of polynomials}

Let \(P_n\) be the vector space of polynomials of degree at most \(n\). The set of polynomials 
\(\{1, x, x^2, \ldots, x^n\}\) is a basis for \(P_n\).
The dimension of \(P_n\) is \(n + 1\).

\begin{itemize}

	\item The set of polynomials \(\{1, x, x^2, \ldots, x^n\}\) is linearly independent.

	\item The set of polynomials \(\{1, x, x^2, \ldots, x^n\}\) spans the vector space \(P_n\).

	\item The dimension of \(P_n\) is \(n + 1\).

\end{itemize}

To prove that the set of polynomials \(\{1, x, x^2, \ldots, x^n\}\) is linearly independent, 
we can use the following steps:

\begin{enumerate}

	\item Assume that there exists a linear combination of the polynomials that equals zero:   

	\[
	    a_0 + a_1 x + a_2 x^2 + \ldots + a_n x^n = 0,
    \]
 		where \(a_0, a_1, \ldots, a_n\) are scalars.
	
	\item Since the left-hand side is a polynomial of degree at most \(n\), it can only be equal 
		  to zero if all coefficients are zero.
 
	\item Therefore, we have \(a_0 = a_1 = \cdots = a_n = 0\), which 
		  proves that the set of polynomials \(\{1, x, x^2, \ldots, x^n\}\) is linearly independent.

\end{enumerate}

So you only have to prove that the set of coefficients vector is linearly independent.

\subsection{Interpolation Polynomial}

Given the \(n+1\) points \((x_k, y_k)\), with \(0 \leq k \leq n\) and all \(x_k\) distinct, 
there exists exactly one polynomial \(p_n \in P_n\) such that \(y_k = p_n(x_k)\) for all 
\(0 \leq k \leq n\). This polynomial is called the interpolation polynomial.

\textbf{Proof:}

The uniqueness follows immediately. We prove the existence by induction on 
\(n\). For \(n = 0\), choose \(p_0(x) = y_0\). 

Now assume the statement is true for \(n-1\). Let the polynomial \(p_{n-1}\)
interpolate the points \((x_0, y_0), \ldots, (x_{n-1}, y_{n-1})\). 

Define

\[
	p_n(x) = p_{n-1}(x) + q(x),
\]

where

\[
	q(x) = \frac{(x - x_0)(x - x_1)\cdots(x - x_{n-1})}{(x_n - x_0)(x_n - x_1)\cdots(x_n - x_{n-1})} 
	(y_n - p_{n-1}(x_n)).
\]

We have \(q \in P_n\), and it follows that \(p_n \in P_n\). 
Furthermore, \(q(x_k) = 0\) for \(k \leq n-1\) because a linear factor in 
the numerator always vanishes at \(x_k\). Therefore, \(p_n(x_k) = y_k\) for 
\(k \leq n-1\). Additionally, we have

\[
	q(x_n) = y_n - p_{n-1}(x_n),
\]

so that \(p_n(x_n) = y_n\). 

\QED

\textbf{Example:}

Consider the three points \((-2, 1)\), \((-1, -1)\), and \((1, 1)\). 
These points uniquely define an interpolating parabola \(p_2\). This parabola can be 
determined using the definition of \(p_n\). 
For hand calculations and a few 
points to interpolate, the following approach is also useful. The general form of the polynomial is 

\[
	p_2(x) = ax^2 + bx + c.
\]

Substituting the three points into this form gives the system of equations:

\[
	1 = a + b + c \quad \text{(from the point (1, 1))}
\]
\[
	-1 = a - b + c \quad \text{(from the point (-1, -1))}
\]
\[
	1 = 4a - 2b + c \quad \text{(from the point (-2, 1))}
\]

This leads to the system of equations:

\[
	\begin{bmatrix}
	1 & 1 & 1 \\
	1 & -1 & 1 \\
	4 & -2 & 1
	\end{bmatrix}
	\begin{bmatrix}
	a \\
	b \\
	c
	\end{bmatrix}
	=
	\begin{bmatrix}
	1 \\
	-1 \\
	1
	\end{bmatrix}
\]

Solving this system gives \(a = 1\), \(b = 1\), and \(c = -1\), so the interpolation polynomial is
\[
	p_2(x) = x^2 + x - 1.
\]

