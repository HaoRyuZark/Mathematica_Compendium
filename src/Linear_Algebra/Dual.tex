\newpage
\section{Dual Spaces}

Given a \(\Field\)-vector space \(V\), we define \(V^*\) \emph{Dual Space} of \(V\), as the set
of linear transformations from \(f: V \to K\). \(f\) is called the \emph{Linear Functional}.

\textbf{Example:}

\(V = \Reals^3, \text{ with } f: V \to \Reals \text{ and } f(x,y,z) = x + y + z\)

\subsection{The Dual Basis}

Given a finite dimensional \(\Field\)-vector space \(V\). Let \(\beta = \{v_1, \dots, v_n\}\) be a
basis of \(V\) let \(f \in V*\). The set of functionals \(\beta^* = \{f_1, \dots, f_n\}\) of
\(V^*\). Defined by:

\[
    f_i(v_j) = \begin{cases}
        1 & \text{ if } j = i   \\
        0 & \text{ if } j \ne i
    \end{cases} = \delta_{ij}
\]

Is called a \emph{Dual Basis}. The definition above is saying that only one of the basis vector
of \(V\) is going to be associated with a function of the basis \(\beta\).

\textbf{Proof:}

To prove a set of vectors is basis we have to prove that they are linearly independent and that
they span the entire vector space.

\emph{Linear Independence:}

We want that

\[
    \sum_{i = 1}^{n} \lambda_i f_i(v) = 0 \text{ only if all } \lambda_i = 0 \text{ for all } v \in V
\]

Now just choose \(n\)-times \(v = v_j, 1 \le j \le n\) and you are going to realize that
because \(f_i(v_j) = 1\) only if \(j = i\) you are going to get \(\lambda_j f_j(v_j) + 0 + \cdots + 0 =
0\). Therefore, by repeating this process you will arrive at the conclusion that all lambdas have
to zero. Thus, the linear Independence is shown.

\emph{Span:}

Suppose \(f \in V^*\) is arbitrary and \(f = \sum_{i = 1}^{n} \lambda_i f_i(v)\) for all \(v \in V\).
Using the previously collected knowledge, which tells us that for \(v = v_j\) the linear
combination of \(\sum_{i = 1}^{n} \lambda_i f_i(v_j)\) is

\[
    \sum_{i = 1}^{n} \lambda_i f_i(v_j) = \lambda_j f_j(v_j) = \lambda_j
\]

Now let us guess \(f = f(v_1)f_1 + \cdots f(v_n)f_n = g \). If \(f(v_i) = g(v_i)\) for all basis
vector of \(V\).

Note for

\[
    g(v_i) = f(v_1)f_1(v_i) + \cdots + f(v_i)f_i(v_i) + \cdots f(v_i)f_n(v_i)
\]

All terms are zero except for \( f(v_i)f_i(v_i) \) which equals \(f(v_i) = g(v_i)\).

\QED

\subsection{How to find a Dual Basis}

We know that to find the coefficients of a linear combination \(f = a_1 f_1 + a_2 f_2 + \cdots + a_n f_n\)
in the space \(V*\) with the basis \(\beta = \{f_1, \dots, f_n\}\) we just need to evaluate the
function \(f\) at each of the basis vectors of the space \(V\) which are \(\{v_1, \dots, v_n\}\).

The process for solving this kind of problems can be summarized into the following steps using
multiple systems of equations.

\begin{enumerate}

    \item For each \(f_i\) repeat the following steps except for the last one.

    \item Use the linearity property of vector to write non-canonical basis vectors as a linear
          combination \(f_i(\vec{k}) = \sum \lambda_k f_i(\vec{v}_k)\), do this for every vector
          in the basis of \(V\) with the current \(f_i\). This will set up the
          left-hand side of the system of equations.

    \item Use the definition of the dual basis for setting up the right-hand side of linear system

    \item Solve the system of equations

    \item If a \(f\) was given then use the found basis to write \(f = \sum_{i = 1}^{n} f(v_i)f_i\)

\end{enumerate}

\textbf{Example:}

Given the space \(V = \Reals^n\) with the basis \(\left\{\begin{bmatrix}
    2 \\ 1 \end{bmatrix}, \begin{bmatrix} 3 \\ 1\end{bmatrix}\right\}\) find \(\beta^* = \{f_1, f_2\}\).

Because of linearity

\begin{align*}
    f_1(2,1) & = 2f_1(\vec{e}_1) + 1 f_1(\vec{e}_2) = 1 \\
    f_1(3,1) & = 3f_1(\vec{e}_1) + 1 f_1(\vec{e}_2) = 0
\end{align*}

We get a linear system of equations. By solving the system we get \(f_1(1, 0) = -1\) and
\(f_2(0, 1) = 3\). Thus, \(f_1(x,y) = x(-1) + y(3)\).

And for \(f_2\) it is the same process

\begin{align*}
    f_2(2,1) & = 2f_1(\vec{e}_1) + 1 f_1(\vec{e}_2) = 0 \\
    f_2(3,1) & = 3f_1(\vec{e}_1) + 1 f_1(\vec{e}_2) = 1
\end{align*}

This time we get \(f_2 = 2x - 5y\). Thus, our basis is \(\beta^* = \{-x + 3y, 2x - 5y\}\), and
if we let f be \(f = 2x -5\) we get that it can be decomposed into:

\begin{align*}
    f & = f(2,1) f_1 + f(3,1) f_2 \\
      & = (-1) f_1 + 1 f_2
\end{align*}


\subsection{Change of Basis in the Dual Space}

Given some finite \(\Field\)-vector space \(V\) with basis \(\beta\) and \(\beta'\). Then matrix for
the change of basis \(\beta^* \to \beta'^*\) is given by

\[
    (T^{-1})^T = (T^T)^{-1}
\]

Where \(T = \beta^{-1}\beta'\).

\textbf{Example: }

Given are \(\beta = \{(1, 1); (-1, 1)\}\) and \(\beta' = \{(2,3), (1,1)\}\).

\[
    T =
    \begin{bmatrix}
        1 & -1 \\
        1 & 1
    \end{bmatrix}^{-1}
    \begin{bmatrix}
        2 & 1 \\
        3 & 1
    \end{bmatrix}
\]

Then

\[
    (T^T)^{-1} =
    \begin{pmatrix}
        0 & 1  \\
        2 & -5
    \end{pmatrix}
\]

\subsection{Dimension Theorem}

If the dimension of \(V\) is finite then \(V\) and \(V^*\) are \emph{isomorphic}.


\subsection{The Transpose of a Linear Transformation}

Let \(V\) and \(W\) be finite dimensional vector spaces with the field \(K\).
Let \(T: V \to W\) be a linear transformation. Now define

\[
    T^T : W^* \to V^*
\]

with an input \(g \in W^*\) where \(g: W \to K\); and an output \(T^T(g) \in V^*\) with
\(T^T(g): V \to K\).
Therefore, \(T^T(g)(\vec{x}) = g (T(\vec{x}))\) for all \(\vec{x} \in V\).

\subsection{Connection to the Transpose Matrix}

Let \(T: V \to W\) be a linear transformation, \(\beta = \{v_1, \dots, v_n\}\)
be a basis of \(V\),  \(\gamma = \{w_1, \dots, w_n\}\) be a basis of \(W\). The matrix of the
change of the transformation is given by \(A = [T]_{\beta}^{\gamma}\) which takes vector from \(\beta\)
to \(\gamma\).

Then let \(\beta^* = \{f_1, \dots, f_n\} \) be the dual basis of \(V^*\) and \(\gamma^* = \{g_1, \dots, g_n\}\) the dual basis
of \(W^*\). Then

\[
    [T^T]_{\gamma^*}^{\beta^*}  = A^T
\]

Which takes functions from \(\gamma^*\) to \(\beta^*\).

\textbf{Proof:}

Because \(T(v_i) \in W\) we can write

\[
    T(v_i) = sum_{k = 1}^{n} A_{ki} w_{k}
\]

as a vector in terms of the basis \(\gamma\).

For \(T^T: W^* \to V^*\) we can write \(T^T(g)\) as a linear combination of the basis vectors \(\beta^*\)

\[
    T^T(g_j) = \sum_{i = 1}^{n} T^T(g_j)(v_i) f_i = \sum_{i = 1}^{n} g(T(v_i)) f_i  = \sum_{i = 1}^{n} A_{ij} f_{i}
\]

where \(A = (T^T(g_1), \dots, T^T(g_n)))\).

We have assumed that \(T^T(g)(\vec{x}) = g (T(\vec{x}))\). Now let us apply this to any \(v_j \in V\)
and \(g_i \in W^*\)

\begin{align*}
    T^T(g_j)(v_i) = g_j(T(v_i))                                                                \\
     & =  g_j \left(\sum_{k = 1}^{n} A_{ki} w_{k}\right) =  \sum_{k = 1}^{n} A_{ki} g_{j}(w_k) \\
     & = A_{ji}
\end{align*}

This sum is equals to \(A_{ji}\) for when \(g_j(w_j)\). Therefore, we have flipped the columns
and rows. Thus, we have shown that

\[
    [T^T]_{\gamma^*}^{\beta^*}  = A^T
\]

Because we have shown it for the basis vectors it gets extended to all of it linear combinations.

\QED

\textbf{Example:}

Let \(P_1 \Reals\) be the space of polynomials of degree at most 1 and \(W = \Reals^2\). Also,
let \(T: V \to W\) be defined by \(T(p) = \begin{bmatrix} p(0) - 2p(1) \\ p(0) + p'(0)\end{bmatrix}\)

For example \(T(1 + 2x) = \begin{bmatrix} 1 - 2(1 + 2) \\ 1 + 2 \end{bmatrix} = \begin{bmatrix} -5 \\ 3 \end{bmatrix}\)

If \(f(a,b) = a - 2b \in W^*\) find \(T^T(f)\).

The output of \(T^T(f)\) is in \(V^*\) and because we are using the transpose we can write it
as \(T^T(f)(p) = f(T(p))\) and

\begin{align*}
    f(a,b)    & = a - 2b \text{ then }             \\
    f(T(p))   & = f((p(0) - 2p(1), p(0) + p'(0)))  \\
              & = (p(0) - 2p(1)) - 2(p(0) + p'(0)) \\
              & = p(0) - 2p(1) - 2p(0) - 2p'(0)    \\
    T^T(f)(p) & = -p(0) - 2p(1) - 2p'(0)
\end{align*}


Now calculate the matrix of \(T\) and \(T^T\). The basis of \(V\) is \(\{1, x\}\) and the basis of \(W\)
is \(\{\begin{bmatrix} 1 \\ 0 \end{bmatrix}, \begin{bmatrix} 0 \\ 1 \end{bmatrix}\}\).

First we know that \(\beta^* = \{ f_1, f_2 \}\) and \(\gamma^* = \{ g_1, g_2 \}\), and we remember that

\[
    f_i(v_j) = \begin{cases}
        1 & \text{ if } j = i   \\
        0 & \text{ if } j \ne i
    \end{cases} = \delta_{ij}
\]

To find the matrix of \(T^T\) which is a \(2 \times 2\)

\[
    T^T =
    \begin{bmatrix}
        a & b \\
        c & d
    \end{bmatrix}
\]

We need to evaluate \(T^T(g_1) \text{ and } T^T(g_2)\) and then write them in terms of \(\beta^*\).

\[
    T^T(g_1) = a f_1 + c f_2
\]

Now we evaluate at the basis vectors of \(V\)

\[
    T^T(g_1)(1) = a f_1(1) + c f_2(1) = a
\]

Now we use the definition of \(T\)

\begin{align*}
    g_1(T(1)) & = g_1((\begin{bmatrix} 1 - 2 \\ 1 + 0 \end{bmatrix}) = g_1(\begin{bmatrix} -1 \\ 1 \end{bmatrix}) \\
              & = g_1(-1\begin{bmatrix} 1 \\ 0 \end{bmatrix} + 1\begin{bmatrix} 0 \\ 1 \end{bmatrix}) = -1
\end{align*}

Thus, we have \(a = -1\).

Now lets repeat this for \(x\)

\[
    T^T(g_1)(x) = a f_1(1) + c f_2(x) = c
\]

\begin{align*}
    g_1(T(x)) & = g_1((\begin{bmatrix} 0 - 2 \\ 0 + 1 \end{bmatrix}) = g_1(\begin{bmatrix} -2 \\ 1 \end{bmatrix}) \\
              & = g_1(-2\begin{bmatrix} 1 \\ 0 \end{bmatrix} + 1\begin{bmatrix} 0 \\ 1 \end{bmatrix}) = -2
\end{align*}

Thus, we have \(c = -2\).

Now if you repeat this for \(g_2\) you will get \(b = 1\) and \(d = 0\).

Thus, the matrix of \(T^T\) is

\[
    T^T =
    \begin{bmatrix}
        -1 & 1 \\
        -2 & 0
    \end{bmatrix}
\]

And the matrix of \(T\) is calculated by applying \(T(1)\) and \(T(x)\) and then writing them in terms
of the basis \(\gamma\).

This gives us

\[
    T(1) = \begin{bmatrix} 1 - 2 \\ 1 + 0 \end{bmatrix} = \begin{bmatrix} -1 \\ 1 \end{bmatrix}
\]

Which is \(-1\begin{bmatrix} 1 \\ 0 \end{bmatrix} + 1\begin{bmatrix} 0 \\ 1 \end{bmatrix}\).

And

\[
    T(x) = \begin{bmatrix} 0 - 2 \\ 0 + 0 \end{bmatrix} = \begin{bmatrix} -2 \\ 0 \end{bmatrix}
\]

Which is \(-2\begin{bmatrix} 1 \\ 0 \end{bmatrix} + 0\begin{bmatrix} 0 \\ 1 \end{bmatrix}\).

Thus, the matrix of \(T\) is

\[
    T =
    \begin{bmatrix}
        -1 & -2 \\
        1  & 0
    \end{bmatrix}
\]


\subsection{The Dual of the Dual Space}

Given a dual space \(V^* = L(V, \Field) \) then 

\[
    (V^*)^* = L(V^*, \Field)
\]


We also define the linear functional \(\varphi: V \to V^{**}\) where \(\varphi(x) = \hat{x}\) and \(\hat{x}(f) = f(x)\) for all \(f \in V\).


\subsubsection{Isomorphism of the Dual Space}

If \(V\) is finite dimensional then \(V^{**}\) and \(V\) are isomorphic. 

\subsubsection{Isomorphism from \(V\) to \(V^{**}\)}

If \(V\) is finite dimensional then the map \(\phi: V \to V^{**}\) is isomorphic. 

\subsection{Basis Theorem for the Dual Space}

Every basis \(\beta^*\) of \(V^*\) is the dual basis of some basis \(\beta\) of \(V\).

\textbf{Proof:}

Let \(\beta^* = \{f_1, \dots, f_n\}\) be a basis of \(V^*\). We can define a basis \(\beta\) of \(V\) with help of the following:

Let us find a basis \(\beta^{**} = \{\mathcal{F}_1, \dots, \mathcal{F}_n\}\) of \(V^{**}\). The functional \(\varphi: V \to V^{**}\) is 
an isomorphism and \(\mathcal{F}_i = \hat{v}_i\) for some \(v_i \in V\). 

Let us choose a basis \(\beta = \{v_1, \dots, v_n\}\) of \(V\) and use \(\beta^*\). By definition 

\[
    f_i(v_j) = \begin{cases}
        1 & \text{ if } j = i   \\
        0 & \text{ if } j \ne i
    \end{cases} = \delta_{ij}
\]

Using the isomorphism \(\varphi\) we get

\[
    f_j(v_i) = \hat{v}_i(f_j) = \mathcal{F}_i(f_j) = \begin{cases}
        1 & \text{ if } j = i   \\
        0 & \text{ if } j \ne i
    \end{cases} = \delta_{ij} 
\]

\QED

\subsection{Transpose of the Transpose is T}

We define \(T^{TT} V^{**} \to W^{**}\) by 

\[
    T^{TT}(\hat{x}) = \hat{x}(T^T)
\]

and claim \(T^{TT} = T\).

\textbf{Proof:}

In this proof we know that \(T^T: W^* \to V^*\). Now let us declare our basis \(\beta = \{v_1, \dots, v_n\}\) for \(V\), \(\gamma = \{w_1, \dots, w_n\} \) for \(W\), 
\(\beta^* = \{f_1, \dots, f_n\}\) for \(V^*\), \(\gamma^* = \{g_1, \dots, g_n\}\) for \(W^*\), \(\beta^{**} =\{\mathcal{F}_1, \dots, \mathcal{F}_n\}\) for \(V^{**}\) and finally \(\gamma^{**} = 
\{\mathcal{G}_1, \dots, \mathcal{G}_n\}\) for \(W^**\).


 When we evaluate at \(g: W \to \Field\) we get 

\begin{align*}
    T^{TT} (\hat{x})(g) & = \hat{x}(T^T)(g)  \\ 
                        & = \hat{x} (T^T(g)) \text{ by associativity }\\
                        & =  T^T (g) (x)     \text{ using the definition of } \hat{x} \\ 
                        & = g(T)(x)          \text{ by associativity }\\
                        & = g(T(x))          \\ 
                        & = \hat{T(x)} (g)
\end{align*}

Here \(\hat{T(x)} \in W^{**}\). Finally 

\begin{align*} 
    T^{TT} (\hat{x})(g) & = \hat{T(x)} (g) \\ 
    T^{TT}(\hat{x}) &= \hat{T(x)} \\ 
    T^{TT}          &= \hat{T} \\
    T^TT &= T \text{ by ignoring the hats }
\end{align*}

They are not strictly the same, but because of the isomorphism we can set them equal.

\QED



